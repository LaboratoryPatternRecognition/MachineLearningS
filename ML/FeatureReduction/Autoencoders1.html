
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Autoencoders &#8212; Dr.Hadi Sadoghi Yazdi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ML/FeatureReduction/Autoencoders1';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Locally Linear Embedding (LLE)" href="LLE.html" />
    <link rel="prev" title="Principal Component Analysis" href="PCA.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../machine_learning.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/Hadi_Sadoghi_Yazdi.png" class="logo__image only-light" alt="Dr.Hadi Sadoghi Yazdi - Home"/>
    <script>document.write(`<img src="../../_static/Hadi_Sadoghi_Yazdi.png" class="logo__image only-dark" alt="Dr.Hadi Sadoghi Yazdi - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../machine_learning.html">
                    Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Regression/NonParamRegression.html">Regression Review</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Regression/KernelRegression.html">Kernel Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Regression/GP_Regression.html">Gaussian Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="FR_Intro.html">Introduction of Feature Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="PCA.html">Principal Component Analysis</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="LLE.html">Locally Linear Embedding (LLE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../EnsembleLearning/EnsembleStart_BaggingBoosting.html">Ensemble Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../EnsembleLearning/Ensemble_Voting_Variant_Bayesian.html">Ensemble by Voting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../EnsembleLearning/TrainableCombiner.html">Trainable Combiners</a></li>
<li class="toctree-l1"><a class="reference internal" href="../EnsembleLearning/FederatedLearning.html">Federated Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../EnsembleLearning/DiffusionNetwork.html">Diffusion Network of Learner</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ContrastiveLearning/ContrastiveLearning_Introduction.html">Contrastive learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../MetricLearning/MetricLearning_1.html">Metric Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../RepresentationLearning/RepresentationLearning1.html">Representation Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ActiveLearning/ActiveLearning_1.html">Active Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../TransferLearning/TransferLearning_1.html">Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../TransferLearning/InstanceBasedTransferLearning1.html">Instance-Based Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FewShotLearning/FewShotLearning_1.html">Few Shot Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../MetaLearning/MetaLearning_1.html">Meta Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Contact_Me.html">Contact Me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../About_Me.html">About Me</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/h-sadoghi/dr-sadoghi.git" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/h-sadoghi/dr-sadoghi.git/issues/new?title=Issue%20on%20page%20%2FML/FeatureReduction/Autoencoders1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/ML/FeatureReduction/Autoencoders1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Autoencoders</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoders-brief">Autoencoders Brief</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#structure-of-autoencoders">Structure of Autoencoders</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ae-formulation">AE formulation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-architecture">Define Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#guide-for-showing-architecture">Guide for Showing Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-features-in-handwritten-digits">Latent features in handwritten digits</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-denoising-mnist-using-ae">Example : Denoising MNIST using AE</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-only">Test Only</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#denoising-with-ae">Denoising with AE</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-algorithm-for-denoising-images-using-an-autoencoder">General Algorithm for Denoising Images Using an Autoencoder</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-autoencoders">Types of Autoencoders</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vanilla-autoencoders"><strong>Vanilla Autoencoders</strong>:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-function-of-the-output-layer">Activation Function of the Output Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-of-f-xi-in-activation-functions">Importance of <span class="math notranslate nohighlight">\( f'(\xi)\)</span> in Activation Functions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#categorization-of-activation-functions-based-on-f-xi">Categorization of Activation Functions Based on <span class="math notranslate nohighlight">\( f'(\xi)\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions-from-perspective-of-derivatives-as-xi">Activation Functions from Perspective of derivatives as <span class="math notranslate nohighlight">\(\xi \)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-scalar-concave-activation-functions-satisfying-lim-xi-to-infty-sigma-xi-0"><em>Continuous Scalar Concave Activation Functions Satisfying <span class="math notranslate nohighlight">\(\lim_{\xi \to \infty} \sigma'(\xi) = 0\)</span></em></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-scalar-concave-activation-function-satisfying-lim-xi-to-infty-sigma-xi-1"><em>Continuous Scalar Concave Activation Function Satisfying <span class="math notranslate nohighlight">\(\lim_{\xi \to \infty} \sigma'(\xi) = 1\)</span></em></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-loss-function-in-autoencoders">Understanding the Loss Function in Autoencoders</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-ae-ellipse-dataset-with-sigmoid-af">Example AE : Ellipse dataset with sigmoid AF</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-ae-ellipse-dataset-with-relu-af">Example AE : Ellipse dataset with RELU AF</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-ae-ellipse-dataset-with-relu-af-with-biasing-data">Example AE : Ellipse dataset with RELU AF with Biasing data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-ae-regulated-sigmoid-for-hidden-and-relu-for-output">Example AE : Regulated Sigmoid for hidden and RELU for output</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#homework-extend-the-previous-work-with-a-3d-sphere">Homework: Extend the Previous Work with a 3D Sphere</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#miniproject-complete-ae-with-binary-cross-entropy-bce">Miniproject complete AE with Binary Cross-Entropy (BCE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-autoencoders"><strong>Convolutional Autoencoders</strong>:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-autoencoders-cae">Convolutional Autoencoders (CAE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#structure-of-a-convolutional-autoencoder">Structure of a Convolutional Autoencoder</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#miniproject-conv-ae">Miniproject: Conv AE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#miniproject-1-2-3-conv-vae">Miniproject 1, 2 , 3: Conv VAE</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="autoencoders">
<h1>Autoencoders<a class="headerlink" href="#autoencoders" title="Link to this heading">#</a></h1>
<section id="autoencoders-brief">
<h2>Autoencoders Brief<a class="headerlink" href="#autoencoders-brief" title="Link to this heading">#</a></h2>
<p>Autoencoders are a type of neural network designed to learn efficient codings of input data. They work by compressing the input into a latent-space representation and then reconstructing the output from this representation. This process involves two main parts: the encoder <span class="math notranslate nohighlight">\( f \)</span> and the decoder <span class="math notranslate nohighlight">\( g \)</span>. The encoder maps the input <span class="math notranslate nohighlight">\( x \)</span> to an internal representation or code <span class="math notranslate nohighlight">\( h \)</span>, while the decoder maps this code <span class="math notranslate nohighlight">\( h \)</span> back to the output <span class="math notranslate nohighlight">\( r \)</span>, which is a reconstruction of the original input.</p>
<p><img alt="AE_GraphNet1" src="../../_images/GraphicalOperationofAE.PNG" /></p>
<p>To further clarify latent space <span class="math notranslate nohighlight">\( h \)</span>, we can use the concept of <strong>Efficient Data Representations</strong>. Consider two sequences: one random and one with a clear pattern (38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16). Despite the first sequence being shorter, the second sequence is easier to memorize because it follows a recognizable pattern of decreasing even numbers. This illustrates why autoencoders are designed to extract and encode such patterns from the input data into the latent space during training.</p>
<p><em><strong>Performing PCA with an Undercomplete Linear Autoencoder</strong></em>
If the autoencoder uses only linear activations and the cost function is the mean squared error (MSE), then it ends up performing Principal Component Analysis.</p>
<section id="structure-of-autoencoders">
<h3>Structure of Autoencoders<a class="headerlink" href="#structure-of-autoencoders" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Encoder</strong>: The encoder compresses the input data into a latent-space representation, reducing its dimensionality. It consists of one or more layers that progressively reduce the size of the input.</p></li>
<li><p><strong>Latent Space</strong>: The compressed representation of the input data, also known as the bottleneck. This part of the network contains the most crucial information needed to reconstruct the original input.</p></li>
<li><p><strong>Decoder</strong>: The decoder reconstructs the input data from the latent representation. It consists of one or more layers that progressively increase the size of the data back to the original input dimensions.</p></li>
</ol>
</section>
<section id="ae-formulation">
<h3>AE formulation<a class="headerlink" href="#ae-formulation" title="Link to this heading">#</a></h3>
<p>The encoder can generally be described as a function <span class="math notranslate nohighlight">\( g \)</span> that depends on certain parameters, denoted as <span class="math notranslate nohighlight">\( h_i = g(x_i) \)</span>, where <span class="math notranslate nohighlight">\( h_i \in \mathbb{R}^q \)</span> represents the latent feature extracted by the encoder block when applied to the input <span class="math notranslate nohighlight">\( x_i \)</span>. Here, the function <span class="math notranslate nohighlight">\( g \)</span> maps from <span class="math notranslate nohighlight">\( \mathbb{R}^n \)</span> to <span class="math notranslate nohighlight">\( \mathbb{R}^q \)</span>.</p>
<p>The decoder, which produces the network’s output denoted by <span class="math notranslate nohighlight">\( \tilde{x}_i \)</span>, is then a function <span class="math notranslate nohighlight">\( f \)</span> of the latent features: <span class="math notranslate nohighlight">\( \tilde{x}_i = f(h_i) = f(g(x_i)) \)</span>, where <span class="math notranslate nohighlight">\( \tilde{x}_i \in \mathbb{R}^n \)</span>.</p>
<p>Training an autoencoder involves finding the functions <span class="math notranslate nohighlight">\( g(·) \)</span> and <span class="math notranslate nohighlight">\( f(·) \)</span> that minimize the difference between the input and output, which is captured by a loss function <span class="math notranslate nohighlight">\( \Delta(x_i, \tilde{x}_i) \)</span>. This loss function penalizes discrepancies between the input <span class="math notranslate nohighlight">\( x_i \)</span> and the reconstructed output <span class="math notranslate nohighlight">\( \tilde{x}_i \)</span>, and the goal is to minimize this loss across all observations.</p>
<p>To avoid an autoencoder learning the identity function, strategies such as applying regularization are used to ensure more meaningful feature learning.</p>
<p><strong>Regularization in Autoencoders</strong></p>
<p>Regularization often involves enforcing sparsity in the latent features. A common approach is to include an <span class="math notranslate nohighlight">\(\ell_1\)</span> or <span class="math notranslate nohighlight">\(\ell_2\)</span> regularization term in the loss function. For <span class="math notranslate nohighlight">\(\ell_2\)</span> regularization, the objective is:</p>
<div class="math notranslate nohighlight">
\[
\argmin_{f, g} \left( \Delta(x_i, \tilde{x}_i) + \lambda \|g(x_i)\|_2^2 \right)
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\Delta(x_i, \tilde{x}_i)\)</span> represents the loss function measuring the difference between the input <span class="math notranslate nohighlight">\(x_i\)</span> and the output <span class="math notranslate nohighlight">\(\tilde{x}_i\)</span>, and <span class="math notranslate nohighlight">\(\|g(x_i)\|_2^2\)</span> is the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm of the latent features with <span class="math notranslate nohighlight">\(\lambda\)</span> as the regularization parameter. The parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> in the functions <span class="math notranslate nohighlight">\(f(·)\)</span> and <span class="math notranslate nohighlight">\(g(·)\)</span> are typically the weights in neural networks.</p>
</section>
</section>
<section id="define-architecture">
<h2>Define Architecture<a class="headerlink" href="#define-architecture" title="Link to this heading">#</a></h2>
<p>There are three key parts of a neural network’s architecture:</p>
<ul class="simple">
<li><p>input, body , output</p></li>
</ul>
<p><img alt="AE_Structure1" src="../../_images/AE_structure.PNG" /></p>
<section id="guide-for-showing-architecture">
<h3>Guide for Showing Architecture<a class="headerlink" href="#guide-for-showing-architecture" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p>Install Graphviz and pydot.</p></li>
<li><p>If you encounter an error while plotting, download Graphviz from <a class="reference external" href="https://graphviz.gitlab.io/download/">https://graphviz.gitlab.io/download/</a>, then extract the files and add the directory to your system’s PATH. For example, if you extract the files to “Program Files,” add the following path:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">C</span><span class="p">:</span>\<span class="n">Program</span> <span class="n">Files</span>\<span class="n">Graphviz</span><span class="o">-</span><span class="mf">12.0.0</span><span class="o">-</span><span class="n">win64</span>\<span class="nb">bin</span>
</pre></div>
</div>
</li>
<li><p>After setting up the PATH, check the following code. The output should be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pydot</span> <span class="ow">and</span> <span class="n">Graphviz</span> <span class="n">are</span> <span class="n">properly</span> <span class="n">installed</span><span class="o">.</span>
</pre></div>
</div>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pydot</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>

<span class="k">def</span> <span class="nf">check_graphviz</span><span class="p">():</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">pydot</span><span class="o">.</span><span class="n">Dot</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">pydot</span><span class="o">.</span><span class="n">Dot</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;pydot and Graphviz are properly installed.&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;An error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">check_graphviz</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">pydot</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="k">def</span> <span class="nf">check_graphviz</span><span class="p">():</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;pydot&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Define Encoder</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder&#39;</span><span class="p">)</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,)))</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>

<span class="c1"># Define Decoder</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder&#39;</span><span class="p">)</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,)))</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>

<span class="c1"># Define Full Autoencoder Model</span>
<span class="c1"># Note: Use Functional API to connect encoder and decoder</span>
<span class="n">input_layer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,))</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_layer</span><span class="p">)</span>
<span class="n">decoded</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">decoded</span><span class="p">)</span>
<span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_dtype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/70d2c25a10d848f967b4f78fc80ec6cbae28df34e16294bb469cefa0b50bf45d.png" src="../../_images/70d2c25a10d848f967b4f78fc80ec6cbae28df34e16294bb469cefa0b50bf45d.png" />
</div>
</div>
</section>
<section id="latent-features-in-handwritten-digits">
<h3>Latent features in handwritten digits<a class="headerlink" href="#latent-features-in-handwritten-digits" title="Link to this heading">#</a></h3>
<p>Latent features in handwritten digits, such as the number and angle of lines needed to form each digit, encapsulate essential information that does not rely on the gray values of each pixel in an image. Humans learn to write by understanding these fundamental components rather than focusing on pixel-level details.</p>
<p><img alt="Latent features in handwritten digits1" src="../../_images/LatentFeatureHandwrittenDigit.PNG" /></p>
</section>
</section>
<section id="example-denoising-mnist-using-ae">
<h2>Example : Denoising MNIST using AE<a class="headerlink" href="#example-denoising-mnist-using-ae" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="kn">import</span> <span class="nn">keras.layers</span> <span class="k">as</span> <span class="nn">L</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="c1"># Load the MNIST dataset from a local .npz file</span>
<span class="n">mnist_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;mnist.npz&#39;</span><span class="p">)</span>

<span class="c1"># Extract the training and test sets</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">mnist_data</span><span class="p">[</span><span class="s1">&#39;x_train&#39;</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">mnist_data</span><span class="p">[</span><span class="s1">&#39;y_train&#39;</span><span class="p">]</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">mnist_data</span><span class="p">[</span><span class="s1">&#39;x_test&#39;</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">mnist_data</span><span class="p">[</span><span class="s1">&#39;y_test&#39;</span><span class="p">]</span>

<span class="c1"># Preprocess the data</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>

<span class="c1"># Filter the dataset to include only digits 0 and 1</span>
<span class="n">train_filter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">test_filter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">train_filter</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">train_filter</span><span class="p">]</span>
<span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="n">test_filter</span><span class="p">],</span> <span class="n">y_test</span><span class="p">[</span><span class="n">test_filter</span><span class="p">]</span>

<span class="c1"># Generate noisy versions of the training data</span>
<span class="n">noise_factor</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">x_train_noisy</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">+</span> <span class="n">noise_factor</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">x_train_noisy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x_train_noisy</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>

<span class="c1"># Combine clean and noisy training data</span>
<span class="n">x_train_combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_train_noisy</span><span class="p">])</span>

<span class="c1"># Define the Autoencoder model</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder&#39;</span><span class="p">)</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Input</span><span class="p">((</span><span class="mi">784</span><span class="p">,)))</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>

<span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder&#39;</span><span class="p">)</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Input</span><span class="p">((</span><span class="mi">32</span><span class="p">,)))</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>

<span class="n">autoencoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">])</span>
<span class="n">autoencoder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">)</span>

<span class="c1"># Train the Autoencoder on the combined data</span>
<span class="n">autoencoder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_combined</span><span class="p">,</span> <span class="n">x_train_combined</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_test</span><span class="p">))</span>

<span class="c1"># Add noise to the test data</span>
<span class="n">x_test_noisy</span> <span class="o">=</span> <span class="n">x_test</span> <span class="o">+</span> <span class="n">noise_factor</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">x_test_noisy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>

<span class="c1"># Denoise the test data</span>
<span class="n">x_test_denoised</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">)</span>

<span class="c1"># Visualize the results</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="c1"># Original</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Original&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

    <span class="c1"># Noisy</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Noisy&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

    <span class="c1"># Denoised</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test_denoised</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Denoised&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">9s</span> 12ms/step - loss: 0.3960 - val_loss: 0.1445
Epoch 2/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.2014 - val_loss: 0.1111
Epoch 3/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1793 - val_loss: 0.1016
Epoch 4/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1716 - val_loss: 0.0932
Epoch 5/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1639 - val_loss: 0.0875
Epoch 6/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1602 - val_loss: 0.0845
Epoch 7/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1569 - val_loss: 0.0814
Epoch 8/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1550 - val_loss: 0.0794
Epoch 9/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 14ms/step - loss: 0.1530 - val_loss: 0.0776
Epoch 10/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1517 - val_loss: 0.0765
Epoch 11/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1500 - val_loss: 0.0756
Epoch 12/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1500 - val_loss: 0.0747
Epoch 13/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1487 - val_loss: 0.0741
Epoch 14/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1480 - val_loss: 0.0735
Epoch 15/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 12ms/step - loss: 0.1474 - val_loss: 0.0729
Epoch 16/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1471 - val_loss: 0.0724
Epoch 17/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1458 - val_loss: 0.0720
Epoch 18/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1460 - val_loss: 0.0716
Epoch 19/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1466 - val_loss: 0.0712
Epoch 20/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1454 - val_loss: 0.0710
Epoch 21/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1452 - val_loss: 0.0704
Epoch 22/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1440 - val_loss: 0.0704
Epoch 23/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1438 - val_loss: 0.0700
Epoch 24/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1439 - val_loss: 0.0696
Epoch 25/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1435 - val_loss: 0.0695
Epoch 26/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1433 - val_loss: 0.0694
Epoch 27/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1434 - val_loss: 0.0690
Epoch 28/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1424 - val_loss: 0.0687
Epoch 29/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1433 - val_loss: 0.0685
Epoch 30/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 12ms/step - loss: 0.1421 - val_loss: 0.0685
Epoch 31/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1424 - val_loss: 0.0682
Epoch 32/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1428 - val_loss: 0.0681
Epoch 33/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1413 - val_loss: 0.0680
Epoch 34/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1409 - val_loss: 0.0680
Epoch 35/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1414 - val_loss: 0.0677
Epoch 36/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1418 - val_loss: 0.0675
Epoch 37/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1414 - val_loss: 0.0678
Epoch 38/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1414 - val_loss: 0.0674
Epoch 39/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1409 - val_loss: 0.0671
Epoch 40/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1404 - val_loss: 0.0672
Epoch 41/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1402 - val_loss: 0.0670
Epoch 42/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 13ms/step - loss: 0.1408 - val_loss: 0.0669
Epoch 43/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1403 - val_loss: 0.0667
Epoch 44/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1410 - val_loss: 0.0667
Epoch 45/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1404 - val_loss: 0.0667
Epoch 46/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1396 - val_loss: 0.0666
Epoch 47/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1397 - val_loss: 0.0665
Epoch 48/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1403 - val_loss: 0.0662
Epoch 49/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1394 - val_loss: 0.0661
Epoch 50/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1395 - val_loss: 0.0666
<span class=" -Color -Color-Bold">67/67</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 2ms/step
</pre></div>
</div>
<img alt="../../_images/addf0360afd6606641303728fabba56086089ba46bfedea1b38f90eb34ac4095.png" src="../../_images/addf0360afd6606641303728fabba56086089ba46bfedea1b38f90eb34ac4095.png" />
</div>
</div>
<section id="test-only">
<h3>Test Only<a class="headerlink" href="#test-only" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">noise_factor</span><span class="o">=</span><span class="mf">0.2</span>
<span class="c1"># Add noise to the test data</span>
<span class="n">x_test_noisy</span> <span class="o">=</span> <span class="n">x_test</span> <span class="o">+</span> <span class="n">noise_factor</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">x_test_noisy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>

<span class="c1"># Denoise the test data</span>
<span class="n">x_test_denoised</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">)</span>

<span class="c1"># Visualize the results</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="c1"># Original</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Original&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

    <span class="c1"># Noisy</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Noisy&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

    <span class="c1"># Denoised</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test_denoised</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Denoised&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">67/67</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 1ms/step
</pre></div>
</div>
<img alt="../../_images/5f3cb015e547f2119f2439b80169d7d62b4bd336c561661a812998e3ef2709f6.png" src="../../_images/5f3cb015e547f2119f2439b80169d7d62b4bd336c561661a812998e3ef2709f6.png" />
</div>
</div>
</section>
</section>
<section id="denoising-with-ae">
<h2>Denoising with AE<a class="headerlink" href="#denoising-with-ae" title="Link to this heading">#</a></h2>
<section id="general-algorithm-for-denoising-images-using-an-autoencoder">
<h3>General Algorithm for Denoising Images Using an Autoencoder<a class="headerlink" href="#general-algorithm-for-denoising-images-using-an-autoencoder" title="Link to this heading">#</a></h3>
<p><strong>Load and Preprocess the Data</strong></p>
<ul class="simple">
<li><p>Load the dataset containing the images. This can be any image dataset, not limited to MNIST.</p></li>
<li><p>Convert the images to grayscale (if necessary) and resize them to a consistent size.</p></li>
<li><p>Normalize the pixel values to be between 0 and 1 by dividing by 255.</p></li>
</ul>
<p><strong>Prepare Noisy and Clean Data</strong></p>
<ul class="simple">
<li><p>Create noisy versions of the images by adding Gaussian noise with varying noise levels.</p></li>
<li><p>Combine the noisy images with the original (clean) images to create training data pairs: <code class="docutils literal notranslate"><span class="pre">(noisy_image,</span> <span class="pre">clean_image)</span></code>.</p></li>
</ul>
<p><strong>Define the Autoencoder Architecture</strong></p>
<ul class="simple">
<li><p><strong>Encoder</strong>: The encoder should reduce the dimensionality of the input images by passing them through several Dense or Convolutional layers with ReLU activation. The output of the encoder is a latent representation of the input image.</p></li>
<li><p><strong>Decoder</strong>: The decoder should reconstruct the clean images from the latent representation by passing the latent space through several Dense or Convolutional layers with ReLU or sigmoid activation.</p></li>
</ul>
<p><strong>Compile the Model</strong></p>
<ul class="simple">
<li><p>Compile the autoencoder model using an optimizer like Adam and a loss function such as binary crossentropy or mean squared error (MSE).</p></li>
</ul>
<p><strong>Train the Autoencoder</strong></p>
<ul class="simple">
<li><p>Train the autoencoder using the pairs of noisy and clean images. Use a validation set for monitoring the model’s performance during training.</p></li>
</ul>
<p><strong>Evaluate the Model</strong></p>
<ul class="simple">
<li><p>After training, test the autoencoder on a separate set of noisy images to evaluate its performance.</p></li>
<li><p>Predict the denoised images using the autoencoder and compare them with the clean images.</p></li>
</ul>
<p><strong>Visualize the Results</strong></p>
<ul class="simple">
<li><p>Visualize a few examples of the original, noisy, and denoised images to qualitatively assess the model’s performance.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="kn">import</span> <span class="nn">keras.layers</span> <span class="k">as</span> <span class="nn">L</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>



<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Custom activation function 1: Example using sine</span>
<span class="k">def</span> <span class="nf">custom_activation1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Custom activation function 2: Example using softplus-like activation</span>
<span class="k">def</span> <span class="nf">custom_activation2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>


<span class="c1"># Load the MNIST dataset from a local .npz file</span>
<span class="n">mnist_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;mnist.npz&#39;</span><span class="p">)</span>

<span class="c1"># Extract the training and test sets</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">mnist_data</span><span class="p">[</span><span class="s1">&#39;x_train&#39;</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">mnist_data</span><span class="p">[</span><span class="s1">&#39;y_train&#39;</span><span class="p">]</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">mnist_data</span><span class="p">[</span><span class="s1">&#39;x_test&#39;</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">mnist_data</span><span class="p">[</span><span class="s1">&#39;y_test&#39;</span><span class="p">]</span>

<span class="c1"># Preprocess the data</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>

<span class="c1"># Filter the dataset to include only digits 0 and 1</span>
<span class="n">train_filter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">test_filter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">train_filter</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">train_filter</span><span class="p">]</span>
<span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="n">test_filter</span><span class="p">],</span> <span class="n">y_test</span><span class="p">[</span><span class="n">test_filter</span><span class="p">]</span>

<span class="c1"># Generate noisy versions of the training data</span>
<span class="n">noise_factor</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">x_train_noisy</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">+</span> <span class="n">noise_factor</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">x_train_noisy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x_train_noisy</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>

<span class="c1"># Combine clean and noisy training data</span>
<span class="n">x_train_combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_train_noisy</span><span class="p">])</span>

<span class="c1"># Define the Autoencoder model with custom activations</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder&#39;</span><span class="p">)</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Input</span><span class="p">((</span><span class="mi">784</span><span class="p">,)))</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">))</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="n">custom_activation1</span><span class="p">))</span>  <span class="c1"># Use custom_activation1</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">))</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="n">custom_activation2</span><span class="p">))</span>  <span class="c1"># Use custom_activation2</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">))</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="n">custom_activation1</span><span class="p">))</span>  <span class="c1"># Use custom_activation1 again</span>

<span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder&#39;</span><span class="p">)</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Input</span><span class="p">((</span><span class="mi">32</span><span class="p">,)))</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">))</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="n">custom_activation2</span><span class="p">))</span>  <span class="c1"># Use custom_activation2</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">))</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="n">custom_activation1</span><span class="p">))</span>  <span class="c1"># Use custom_activation1</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>  <span class="c1"># Use sigmoid for the output layer</span>

<span class="c1"># Compile the Autoencoder model</span>
<span class="n">autoencoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">])</span>
<span class="n">autoencoder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">)</span>

<span class="c1"># Train the Autoencoder on the combined data</span>
<span class="n">autoencoder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_combined</span><span class="p">,</span> <span class="n">x_train_combined</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_test</span><span class="p">))</span>

<span class="c1"># Add noise to the test data</span>
<span class="n">x_test_noisy</span> <span class="o">=</span> <span class="n">x_test</span> <span class="o">+</span> <span class="n">noise_factor</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">x_test_noisy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>

<span class="c1"># Denoise the test data</span>
<span class="n">x_test_denoised</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">)</span>

<span class="c1"># Visualize the results</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="c1"># Original</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Original&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

    <span class="c1"># Noisy</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Noisy&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

    <span class="c1"># Denoised</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test_denoised</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Denoised&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:From e:\HadiSadoghiYazdi\.M_HomePage\Lib\site-packages\keras\src\backend\tensorflow\core.py:192: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Epoch 1/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">3s</span> 12ms/step - loss: 0.3793 - val_loss: 0.1709
Epoch 2/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.2190 - val_loss: 0.1298
Epoch 3/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1907 - val_loss: 0.1154
Epoch 4/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1832 - val_loss: 0.1079
Epoch 5/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1755 - val_loss: 0.0999
Epoch 6/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1699 - val_loss: 0.0945
Epoch 7/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1648 - val_loss: 0.0907
Epoch 8/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1613 - val_loss: 0.0874
Epoch 9/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1586 - val_loss: 0.0842
Epoch 10/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1553 - val_loss: 0.0813
Epoch 11/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1541 - val_loss: 0.0792
Epoch 12/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1518 - val_loss: 0.0775
Epoch 13/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1503 - val_loss: 0.0762
Epoch 14/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1494 - val_loss: 0.0749
Epoch 15/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1487 - val_loss: 0.0740
Epoch 16/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1476 - val_loss: 0.0731
Epoch 17/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 12ms/step - loss: 0.1464 - val_loss: 0.0726
Epoch 18/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1464 - val_loss: 0.0718
Epoch 19/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 12ms/step - loss: 0.1447 - val_loss: 0.0713
Epoch 20/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 13ms/step - loss: 0.1450 - val_loss: 0.0707
Epoch 21/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1444 - val_loss: 0.0703
Epoch 22/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1439 - val_loss: 0.0700
Epoch 23/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1439 - val_loss: 0.0695
Epoch 24/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1428 - val_loss: 0.0691
Epoch 25/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1417 - val_loss: 0.0688
Epoch 26/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1414 - val_loss: 0.0686
Epoch 27/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 9ms/step - loss: 0.1411 - val_loss: 0.0683
Epoch 28/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1413 - val_loss: 0.0683
Epoch 29/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1410 - val_loss: 0.0680
Epoch 30/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 12ms/step - loss: 0.1415 - val_loss: 0.0676
Epoch 31/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 13ms/step - loss: 0.1409 - val_loss: 0.0675
Epoch 32/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 12ms/step - loss: 0.1411 - val_loss: 0.0674
Epoch 33/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 12ms/step - loss: 0.1402 - val_loss: 0.0671
Epoch 34/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 12ms/step - loss: 0.1409 - val_loss: 0.0669
Epoch 35/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 12ms/step - loss: 0.1395 - val_loss: 0.0667
Epoch 36/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1408 - val_loss: 0.0666
Epoch 37/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 13ms/step - loss: 0.1400 - val_loss: 0.0665
Epoch 38/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 16ms/step - loss: 0.1396 - val_loss: 0.0663
Epoch 39/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 13ms/step - loss: 0.1392 - val_loss: 0.0661
Epoch 40/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 14ms/step - loss: 0.1397 - val_loss: 0.0660
Epoch 41/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 13ms/step - loss: 0.1399 - val_loss: 0.0659
Epoch 42/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 14ms/step - loss: 0.1391 - val_loss: 0.0659
Epoch 43/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1390 - val_loss: 0.0657
Epoch 44/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1390 - val_loss: 0.0656
Epoch 45/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 11ms/step - loss: 0.1392 - val_loss: 0.0656
Epoch 46/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1385 - val_loss: 0.0653
Epoch 47/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1384 - val_loss: 0.0653
Epoch 48/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1393 - val_loss: 0.0652
Epoch 49/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1385 - val_loss: 0.0652
Epoch 50/50
<span class=" -Color -Color-Bold">99/99</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 10ms/step - loss: 0.1388 - val_loss: 0.0652
<span class=" -Color -Color-Bold">67/67</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 2ms/step
</pre></div>
</div>
<img alt="../../_images/f109db596b0f2ce251bdd8f48453bf0eb398d41c3fefcb8d3319668c204db0f7.png" src="../../_images/f109db596b0f2ce251bdd8f48453bf0eb398d41c3fefcb8d3319668c204db0f7.png" />
</div>
</div>
</section>
</section>
<section id="types-of-autoencoders">
<h2>Types of Autoencoders<a class="headerlink" href="#types-of-autoencoders" title="Link to this heading">#</a></h2>
<section id="vanilla-autoencoders">
<h3><strong>Vanilla Autoencoders</strong>:<a class="headerlink" href="#vanilla-autoencoders" title="Link to this heading">#</a></h3>
<p>The basic form of autoencoders, consisting of a simple encoder-decoder architecture with fully connected layers. it appeared above which is a specific type of feedforward neural networks where the input is the same as the output.</p>
<p><img alt="Vanilla_FFNN1" src="../../_images/Vanilla_FeedForwardNN.PNG" /></p>
<section id="activation-function-of-the-output-layer">
<h4>Activation Function of the Output Layer<a class="headerlink" href="#activation-function-of-the-output-layer" title="Link to this heading">#</a></h4>
<p>The <strong>ReLU activation function</strong> can assume all values in the range <span class="math notranslate nohighlight">\([0, \infty)\)</span>. As a reminder, its formula is:</p>
<div class="math notranslate nohighlight">
\[
\text{ReLU}(x) = \max(0, x)
\]</div>
<p>the input observations <span class="math notranslate nohighlight">\(x_i\)</span>. If the input <span class="math notranslate nohighlight">\(x_i\)</span> be negative values, the ReLU might not be the optimal choice, as it outputs zero for all negative inputs, potentially leading to the problem of “dead neurons.” In such cases, other activation functions, such as the <strong>Leaky ReLU</strong>, can be more suitable. The Leaky ReLU allows a small, non-zero gradient when the input is negative, defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{Leaky ReLU}(x) = \begin{cases} 
x &amp; \text{if } x &gt; 0 \\
\alpha x &amp; \text{if } x \leq 0
\end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is a small constant, typically 0.01.</p>
<p>The <strong>Sigmoid function</strong> output of <span class="math notranslate nohighlight">\(\sigma(x)\)</span> is in the range <span class="math notranslate nohighlight">\((0, 1)\)</span>. Its formula is:</p>
<div class="math notranslate nohighlight">
\[
\f(x) = \frac{1}{1 + e^{-x}}
\]</div>
<p>This function is useful when the output needs to be interpreted as a probability. However, it suffers from the vanishing gradient problem, making it less suitable for deep networks.</p>
<p>The <strong>Hyperbolic Tangent (tanh)</strong> function is another common activation function that scales the output to the range <span class="math notranslate nohighlight">\([-1, 1]\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]</div>
<p>The tanh function is centered at zero, often leading to faster convergence during training compared to the sigmoid function.</p>
<p>The <strong>ELU (Exponential Linear Unit)</strong> is an activation function that tries to improve the ReLU by reducing the bias shift, which can occur during training. The formula for ELU is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{ELU}(x) = \begin{cases} 
x &amp; \text{if } x &gt; 0 \\
\alpha (e^x - 1) &amp; \text{if } x \leq 0
\end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is a positive constant, typically 1. ELU helps in learning faster and producing better accuracy in deep networks by smoothing the output for negative inputs.</p>
<p>The <strong>Swish function</strong>, developed by Google, is another activation function that has shown promising results. It is defined as:</p>
<div class="math notranslate nohighlight">
\[
\text{Swish}(x) = \frac{x}{1 + e^{-x}} = x \cdot \f(x)
\]</div>
<p>Swish is a smooth, non-monotonic function that often performs better than ReLU in deep networks.</p>
<p>Finally, the <strong>GELU (Gaussian Error Linear Unit)</strong> is an activation function that applies a Gaussian distribution to the inputs, defined as:</p>
<div class="math notranslate nohighlight">
\[
\text{GELU}(x) = x \cdot \Phi(x)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi(x)\)</span> is the cumulative distribution function of the standard Gaussian distribution. GELU combines the benefits of ReLU and dropout, leading to improved performance in many deep learning tasks. It have some properties relative RELU suach as (No Dead Neurons, Smooth Gradient)
(<strong>Dropout</strong> is a regularization technique that randomly drops (sets to zero) a fraction of the neurons during training to prevent overfitting. It does this by forcing the network to learn more robust features and avoid relying too heavily on specific neurons.)</p>
<p>We mentioned to the <strong>Softmax function</strong>
(Softmax function is generally not a good choice for an autoencoder’s activation function) is typically used in the output layer of neural networks for multi-class classification problems.
It converts the logits into probabilities that sum to 1, where each class’s probability is given by:</p>
<div class="math notranslate nohighlight">
\[
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}
\]</div>
<p>This function is particularly useful when the model needs to predict multiple classes simultaneously.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the activation functions</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">e_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># for numerical stability</span>
    <span class="k">return</span> <span class="n">e_x</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">e_x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">swish</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.044715</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">))))</span>

<span class="c1"># Generate input data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ReLU&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Leaky ReLU&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sigmoid&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Tanh&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Softmax&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">elu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ELU&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">swish</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Swish&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;GELU&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/6d48712b9916dd26c496016247421b13e2e0f7cc9082eb815ab0c08198a315b3.png" src="../../_images/6d48712b9916dd26c496016247421b13e2e0f7cc9082eb815ab0c08198a315b3.png" />
</div>
</div>
</section>
<section id="importance-of-f-xi-in-activation-functions">
<h4>Importance of <span class="math notranslate nohighlight">\( f'(\xi)\)</span> in Activation Functions<a class="headerlink" href="#importance-of-f-xi-in-activation-functions" title="Link to this heading">#</a></h4>
<p>The derivative of an activation function, denoted as <span class="math notranslate nohighlight">\(f'(\xi)\)</span>, is crucial for several reasons in the context of neural networks:</p>
<p><strong>Gradient Computation</strong>:</p>
<ul class="simple">
<li><p><strong>Backpropagation</strong>: The derivative <span class="math notranslate nohighlight">\(f'(\xi)\)</span> is essential for backpropagation, the algorithm used to train neural networks. It helps compute the gradient of the loss function with respect to the weights of the network. This gradient is used to update the weights during training.</p></li>
<li><p><strong>Learning Dynamics</strong>: The value of <span class="math notranslate nohighlight">\( f'(\xi)\)</span> determines how effectively the network learns. If <span class="math notranslate nohighlight">\( f'(\xi)\)</span> is too small (as in the case of saturated activation functions), gradients can vanish, making learning slow or ineffective. If <span class="math notranslate nohighlight">\( f'(\xi)\)</span> is too large, gradients can explode, leading to instability during training.</p></li>
</ul>
<p><strong>Non-Linearity and Capacity</strong>:</p>
<ul class="simple">
<li><p><strong>Introducing Non-Linearity</strong>: Activation functions introduce non-linearity into the network, allowing it to learn complex patterns. The shape of <span class="math notranslate nohighlight">\( f'(\xi)\)</span> affects the non-linearity introduced by the activation function.</p></li>
</ul>
</section>
</section>
<section id="categorization-of-activation-functions-based-on-f-xi">
<h3>Categorization of Activation Functions Based on <span class="math notranslate nohighlight">\( f'(\xi)\)</span><a class="headerlink" href="#categorization-of-activation-functions-based-on-f-xi" title="Link to this heading">#</a></h3>
<p>The categorization of activation functions based on their derivative behavior (<span class="math notranslate nohighlight">\( f'(\xi)\)</span>) helps understand their suitability for different tasks:</p>
<ul class="simple">
<li><p><strong>Functions with <span class="math notranslate nohighlight">\(\lim_{\xi \to \infty} f'(\xi) = 0\)</span></strong>:</p>
<ul>
<li><p>These functions have derivatives that approach zero for large <span class="math notranslate nohighlight">\(\xi\)</span>, indicating that the function saturates at high values. This saturation can prevent gradients from becoming excessively large but may also lead to vanishing gradients, making them less suitable for deep networks.</p></li>
<li><p>Examples:</p>
<ul>
<li><p><strong>Sigmoid</strong>: <span class="math notranslate nohighlight">\( f(\xi) = \frac{1}{1 + \exp(-\xi)}\)</span></p></li>
<li><p><strong>Hyperbolic Tangent (Tanh)</strong>: <span class="math notranslate nohighlight">\( f(\xi) = \tanh(\xi)\)</span></p></li>
<li><p><strong>Logarithmic</strong>: <span class="math notranslate nohighlight">\( f(\xi) = \log(1 + \xi)\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Functions with <span class="math notranslate nohighlight">\(\lim_{\xi \to \infty} f'(\xi) = 1\)</span></strong>:</p>
<ul>
<li><p>These functions have derivatives that approach 1 for large <span class="math notranslate nohighlight">\(\xi\)</span>, meaning their rate of change remains more consistent. This behavior helps in maintaining gradient flow during training and can be advantageous for avoiding vanishing gradients.</p></li>
<li><p>Examples:</p>
<ul>
<li><p><strong>ReLU</strong>: <span class="math notranslate nohighlight">\( f(\xi) = \max(0, \xi)\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<section id="activation-functions-from-perspective-of-derivatives-as-xi">
<h4>Activation Functions from Perspective of derivatives as <span class="math notranslate nohighlight">\(\xi \)</span><a class="headerlink" href="#activation-functions-from-perspective-of-derivatives-as-xi" title="Link to this heading">#</a></h4>
<p>Continuous scalar concave activation functions into two classes based on the behavior of their derivatives as <span class="math notranslate nohighlight">\(\xi \to \infty\)</span>:</p>
<section id="continuous-scalar-concave-activation-functions-satisfying-lim-xi-to-infty-sigma-xi-0">
<h5><em>Continuous Scalar Concave Activation Functions Satisfying <span class="math notranslate nohighlight">\(\lim_{\xi \to \infty} \sigma'(\xi) = 0\)</span></em><a class="headerlink" href="#continuous-scalar-concave-activation-functions-satisfying-lim-xi-to-infty-sigma-xi-0" title="Link to this heading">#</a></h5>
<p>These functions have derivatives that approach zero as <span class="math notranslate nohighlight">\(\xi\)</span> grows large, meaning their rate of change diminishes in the positive direction. They tend to saturate at large values of <span class="math notranslate nohighlight">\(\xi\)</span>. Examples include:</p>
<p><strong>Sigmoid Function</strong>:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\xi) = \frac{1}{1 + \exp(-\xi)}
\]</div>
<ul class="simple">
<li><p><strong>Derivative</strong>: <span class="math notranslate nohighlight">\(\sigma'(\xi) = \sigma(\xi)(1 - \sigma(\xi))\)</span>, which approaches 0 as <span class="math notranslate nohighlight">\(\xi \to \infty\)</span> or <span class="math notranslate nohighlight">\(\xi \to -\infty\)</span>.</p></li>
</ul>
<p><strong>Capped ReLU</strong>:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\xi) = \min(\xi, \beta), \quad \beta &gt; 0
\]</div>
<ul class="simple">
<li><p><strong>Derivative</strong>: <span class="math notranslate nohighlight">\(\sigma'(\xi) = 1\)</span> for <span class="math notranslate nohighlight">\(0 \leq \xi \leq \beta\)</span>, and <span class="math notranslate nohighlight">\(0\)</span> for <span class="math notranslate nohighlight">\(\xi &gt; \beta\)</span>.</p></li>
</ul>
<p><strong>Saturated Linear</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\sigma(\xi) = 
\begin{cases} 
1, &amp; \text{if } \xi &gt; 1 \\
\xi, &amp; \text{if } 0 \leq \xi \leq 1 
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p><strong>Derivative</strong>: <span class="math notranslate nohighlight">\(\sigma'(\xi) = 1\)</span> for <span class="math notranslate nohighlight">\(0 \leq \xi \leq 1\)</span>, and <span class="math notranslate nohighlight">\(0\)</span> for <span class="math notranslate nohighlight">\(\xi &gt; 1\)</span>.</p></li>
</ul>
<p><strong>Inverse Square Root Unit</strong>:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\xi) = \frac{\sqrt{\xi}}{1 + \xi^2}
\]</div>
<ul class="simple">
<li><p><strong>Derivative</strong>: Approaches 0 as <span class="math notranslate nohighlight">\(\xi \to \infty\)</span>.</p></li>
</ul>
<p><strong>Arctangent Function</strong>:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\xi) = \frac{2}{\pi} \arctan(\xi)
\]</div>
<ul class="simple">
<li><p><strong>Derivative</strong>: <span class="math notranslate nohighlight">\(\sigma'(\xi) = \frac{2}{\pi} \cdot \frac{1}{1 + \xi^2}\)</span>, which approaches 0 as <span class="math notranslate nohighlight">\(\xi \to \infty\)</span>.</p></li>
</ul>
<p><strong>Hyperbolic Tangent</strong>:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\xi) = \tanh(\xi)
\]</div>
<ul class="simple">
<li><p><strong>Derivative</strong>: <span class="math notranslate nohighlight">\(\sigma'(\xi) = 1 - \tanh^2(\xi)\)</span>, which approaches 0 as <span class="math notranslate nohighlight">\(\xi \to \infty\)</span>.</p></li>
</ul>
<p><strong>Inverse Hyperbolic Sine</strong>:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\xi) = \text{arcsinh}(\xi)
\]</div>
<ul class="simple">
<li><p><strong>Derivative</strong>: <span class="math notranslate nohighlight">\(\sigma'(\xi) = \frac{1}{\sqrt{\xi^2 + 1}}\)</span>, which approaches 0 as <span class="math notranslate nohighlight">\(\xi \to \infty\)</span>.</p></li>
</ul>
<p><strong>Elliot Function</strong>:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\xi) = \frac{\xi}{1 + \xi}
\]</div>
<ul class="simple">
<li><p><strong>Derivative</strong>: <span class="math notranslate nohighlight">\(\sigma'(\xi) = \frac{1}{(1 + \xi)^2}\)</span>, which approaches 0 as <span class="math notranslate nohighlight">\(\xi \to \infty\)</span>.</p></li>
</ul>
<p><strong>Logarithmic Function</strong>:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\xi) = \log(1 + \xi)
\]</div>
<ul class="simple">
<li><p><strong>Derivative</strong>: <span class="math notranslate nohighlight">\(\sigma'(\xi) = \frac{1}{1 + \xi}\)</span>, which approaches 0 as <span class="math notranslate nohighlight">\(\xi \to \infty\)</span>.</p></li>
</ul>
</section>
<section id="continuous-scalar-concave-activation-function-satisfying-lim-xi-to-infty-sigma-xi-1">
<h5><em>Continuous Scalar Concave Activation Function Satisfying <span class="math notranslate nohighlight">\(\lim_{\xi \to \infty} \sigma'(\xi) = 1\)</span></em><a class="headerlink" href="#continuous-scalar-concave-activation-function-satisfying-lim-xi-to-infty-sigma-xi-1" title="Link to this heading">#</a></h5>
<p>These functions have derivatives that approach 1 as <span class="math notranslate nohighlight">\(\xi \to \infty\)</span>, meaning their rate of change remains constant and does not saturate.</p>
<p><strong>ReLU (Rectified Linear Unit)</strong>:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\xi) = \xi
\]</div>
<ul class="simple">
<li><p><strong>Derivative</strong>: <span class="math notranslate nohighlight">\(\sigma'(\xi) = 1\)</span> for <span class="math notranslate nohighlight">\(\xi \geq 0\)</span>, and 0 for <span class="math notranslate nohighlight">\(\xi &lt; 0\)</span>.</p></li>
</ul>
</section>
</section>
<section id="understanding-the-loss-function-in-autoencoders">
<h4>Understanding the Loss Function in Autoencoders<a class="headerlink" href="#understanding-the-loss-function-in-autoencoders" title="Link to this heading">#</a></h4>
<p>In autoencoders, the loss function is crucial as it measures the difference between the input <span class="math notranslate nohighlight">\( \mathbf{x}_i \)</span> and the reconstructed output <span class="math notranslate nohighlight">\( \hat{\mathbf{x}}_i \)</span>. The goal is to minimize this difference, which means the autoencoder is trying to learn an approximation of the identity function, mapping inputs as closely as possible to themselves.</p>
<p>The loss function is defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\mathbf{x}_i, \hat{\mathbf{x}}_i) = \Delta(\mathbf{x}_i, \hat{\mathbf{x}}_i),
\]</div>
<p>where <span class="math notranslate nohighlight">\( \Delta(\cdot) \)</span> is a metric used to measure the difference between <span class="math notranslate nohighlight">\( \mathbf{x}_i \)</span> and <span class="math notranslate nohighlight">\( \hat{\mathbf{x}}_i \)</span>.</p>
<p>Two common loss functions used in autoencoders are <strong>Mean Squared Error (MSE)</strong> and <strong>Binary Cross-Entropy (BCE)</strong>.</p>
<p><strong>Minimizing Mean Squared Error (MSE):</strong></p>
<p>To solve this problem, we’ll consider a simple autoencoder with the following structure:</p>
<p><strong>Input Layer:</strong> <span class="math notranslate nohighlight">\( \mathbf{x} \)</span> is a 3-dimensional vector.
<strong>Hidden Layer (encoding):</strong> <span class="math notranslate nohighlight">\( \mathbf{z} \)</span> is a 2-dimensional vector. The transformation from <span class="math notranslate nohighlight">\( \mathbf{x} \)</span> to <span class="math notranslate nohighlight">\( \mathbf{z} \)</span> involves linear weights and a sigmoid activation function.
<strong>Output Layer (decoding):</strong> The reconstruction <span class="math notranslate nohighlight">\( \hat{\mathbf{x}} \)</span> is a 3-dimensional vector, achieved through linear weights and a sigmoid activation function.</p>
<p><strong>Define the Encoding and Decoding Functions</strong></p>
<p>Let <span class="math notranslate nohighlight">\( \mathbf{x} = [x_1, x_2, x_3]^\top \)</span> be the input vector.</p>
<p><strong>Encoding:</strong> The encoded representation <span class="math notranslate nohighlight">\( \mathbf{z} \)</span> is computed as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z} = \sigma(W_e \mathbf{x} + \mathbf{b}_e),
\]</div>
<p>where <span class="math notranslate nohighlight">\( W_e \)</span> is a <span class="math notranslate nohighlight">\( 2 \times 3 \)</span> matrix of weights for the encoding layer, <span class="math notranslate nohighlight">\( \mathbf{b}_e \)</span> is a <span class="math notranslate nohighlight">\( 2 \times 1 \)</span> bias vector, and <span class="math notranslate nohighlight">\( \sigma(\cdot) \)</span> is the sigmoid activation function.</p>
<p>Specifically, let:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
W_e = \begin{pmatrix}
w_{11} &amp; w_{12} &amp; w_{13} \\
w_{21} &amp; w_{22} &amp; w_{23}
\end{pmatrix}, \quad \mathbf{b}_e = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix},
\end{split}\]</div>
<p>then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{z} = \begin{pmatrix}
z_1 \\
z_2
\end{pmatrix} = \sigma\left( \begin{pmatrix}
w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + b_1 \\
w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + b_2
\end{pmatrix} \right).
\end{split}\]</div>
<p><strong>Decoding:</strong> The decoded output <span class="math notranslate nohighlight">\( \hat{\mathbf{x}} \)</span> is computed as:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathbf{x}} = \sigma(W_d \mathbf{z} + \mathbf{b}_d),
\]</div>
<p>where <span class="math notranslate nohighlight">\( W_d \)</span> is a <span class="math notranslate nohighlight">\( 3 \times 2 \)</span> matrix of weights for the decoding layer, <span class="math notranslate nohighlight">\( \mathbf{b}_d \)</span> is a <span class="math notranslate nohighlight">\( 3 \times 1 \)</span> bias vector, and <span class="math notranslate nohighlight">\( \sigma(\cdot) \)</span> is again the sigmoid activation function.</p>
<p>Specifically, let:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
W_d = \begin{pmatrix}
w_{11}' &amp; w_{12}' \\
w_{21}' &amp; w_{22}' \\
w_{31}' &amp; w_{32}'
\end{pmatrix}, \quad \mathbf{b}_d = \begin{pmatrix} b_1' \\ b_2' \\ b_3' \end{pmatrix},
\end{split}\]</div>
<p>then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{\mathbf{x}} = \begin{pmatrix}
\hat{x}_1 \\
\hat{x}_2 \\
\hat{x}_3
\end{pmatrix} = \sigma\left( \begin{pmatrix}
w_{11}'z_1 + w_{12}'z_2 + b_1' \\
w_{21}'z_1 + w_{22}'z_2 + b_2' \\
w_{31}'z_1 + w_{32}'z_2 + b_3'
\end{pmatrix} \right).
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">draw_autoencoder</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="c1"># Define positions of nodes</span>
    <span class="n">input_positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)])</span>
    <span class="n">hidden_positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>  <span class="c1"># Adjust y-position for hidden layer</span>
    <span class="n">output_positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)])</span>

    <span class="c1"># Plot nodes</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">input_positions</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">input_positions</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Input Layer&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">hidden_positions</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">hidden_positions</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Hidden Layer&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">output_positions</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">output_positions</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Output Layer&#39;</span><span class="p">)</span>

    <span class="c1"># Draw connections with weights</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">input_pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_positions</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">hidden_pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden_positions</span><span class="p">):</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">weights_input_hidden</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">input_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">input_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
                    <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>  <span class="c1"># Reduce line thickness, ensure min thickness</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">((</span><span class="n">input_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">hidden_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">input_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">hidden_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span>
                    <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">weight</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>  <span class="c1"># Increase font size</span>

    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">hidden_pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden_positions</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">output_pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_positions</span><span class="p">):</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">weights_hidden_output</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">hidden_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">output_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">hidden_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
                    <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>  <span class="c1"># Reduce line thickness, ensure min thickness</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">((</span><span class="n">hidden_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span>
                    <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">weight</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>  <span class="c1"># Increase font size</span>

    <span class="c1"># Add labels and legend</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">input_positions</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">input_positions</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;X</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">hidden_positions</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">hidden_positions</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;H</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">output_positions</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">output_positions</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;Y</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feedforward Autoencoder Network&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Layer&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Node&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Initialize weights</span>
<span class="n">weights_input_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># 3 input nodes to 2 hidden nodes</span>
<span class="n">weights_hidden_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># 2 hidden nodes to 3 output nodes</span>

<span class="c1"># Draw the network</span>
<span class="n">draw_autoencoder</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/d0dd7a8d47b8c9d82fafe37766de886f14eec31b5787df58e44ade994f07aeae.png" src="../../_images/d0dd7a8d47b8c9d82fafe37766de886f14eec31b5787df58e44ade994f07aeae.png" />
</div>
</div>
<p><strong>Define the Loss Function</strong></p>
<p>The loss function is defined as the Mean Squared Error (MSE) between the input <span class="math notranslate nohighlight">\( \mathbf{x} \)</span> and the reconstructed output <span class="math notranslate nohighlight">\( \hat{\mathbf{x}} \)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{MSE}} = \frac{1}{M} \sum_{i=1}^{M} \|\mathbf{x}_i - \hat{\mathbf{x}}_i\|^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\( M \)</span> is the number of training samples.</p>
<p>For a single training example <span class="math notranslate nohighlight">\( i \)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{MSE}}^{(i)} = \frac{1}{2} \sum_{j=1}^{3} (x_{i,j} - \hat{x}_{i,j})^2.
\]</div>
<p><strong>Compute the Derivatives</strong></p>
<p>To find the optimum weights, we’ll calculate the gradient of the loss function with respect to the weights in both the encoding and decoding layers.</p>
<p><strong>Derivatives with respect to Decoding Weights <span class="math notranslate nohighlight">\( W_d \)</span></strong></p>
<p>Let’s start by computing the partial derivative of <span class="math notranslate nohighlight">\( \mathcal{L}_{\text{MSE}} \)</span> with respect to a weight <span class="math notranslate nohighlight">\( w_{jk}' \)</span> in the decoding layer:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial w_{jk}'} = \frac{1}{M} \sum_{i=1}^{M} \frac{\partial \mathcal{L}_{\text{MSE}}^{(i)}}{\partial w_{jk}'}.
\]</div>
<p>Since:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{MSE}}^{(i)} = \frac{1}{2} \sum_{j=1}^{3} (x_{i,j} - \hat{x}_{i,j})^2,
\]</div>
<p>we need:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}_{\text{MSE}}^{(i)}}{\partial w_{jk}'} = \sum_{j=1}^{3} (x_{i,j} - \hat{x}_{i,j}) \cdot \frac{\partial \hat{x}_{i,j}}{\partial w_{jk}'}.
\]</div>
<p>The partial derivative of <span class="math notranslate nohighlight">\( \hat{x}_{i,j} \)</span> with respect to <span class="math notranslate nohighlight">\( w_{jk}' \)</span> involves the sigmoid function:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \hat{x}_{i,j}}{\partial w_{jk}'} = \frac{\partial \sigma\left( \sum_{k=1}^{2} w_{jk}' z_{i,k} + b_j' \right)}{\partial w_{jk}'}.
\]</div>
<p>Using the chain rule:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \hat{x}_{i,j}}{\partial w_{jk}'} = \sigma'(a_{i,j}') \cdot z_{i,k},
\]</div>
<p>where <span class="math notranslate nohighlight">\( a_{i,j}' = \sum_{k=1}^{2} w_{jk}' z_{i,k} + b_j' \)</span> and <span class="math notranslate nohighlight">\( \sigma'(a_{i,j}') = \hat{x}_{i,j}(1 - \hat{x}_{i,j}) \)</span>.</p>
<p>So:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}_{\text{MSE}}^{(i)}}{\partial w_{jk}'} = -\sum_{j=1}^{3} (x_{i,j} - \hat{x}_{i,j}) \cdot \hat{x}_{i,j}(1 - \hat{x}_{i,j}) \cdot z_{i,k}.
\]</div>
<p><strong>Derivatives with respect to Encoding Weights <span class="math notranslate nohighlight">\( W_e \)</span></strong></p>
<p>Next, we calculate the partial derivative of <span class="math notranslate nohighlight">\( \mathcal{L}_{\text{MSE}} \)</span> with respect to a weight <span class="math notranslate nohighlight">\( w_{mn} \)</span> in the encoding layer. This is more complex due to the dependency on <span class="math notranslate nohighlight">\( \mathbf{z} \)</span>, which is used in the decoding step.</p>
<p>Start with:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial w_{mn}} = \frac{1}{M} \sum_{i=1}^{M} \frac{\partial \mathcal{L}_{\text{MSE}}^{(i)}}{\partial w_{mn}}.
\]</div>
<p>Apply the chain rule:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}_{\text{MSE}}^{(i)}}{\partial w_{mn}} = \sum_{j=1}^{3} \frac{\partial \mathcal{L}_{\text{MSE}}^{(i)}}{\partial \hat{x}_{i,j}} \cdot \frac{\partial \hat{x}_{i,j}}{\partial z_{i,m}} \cdot \frac{\partial z_{i,m}}{\partial w_{mn}}.
\]</div>
<p>Using:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}_{\text{MSE}}^{(i)}}{\partial \hat{x}_{i,j}} = -(x_{i,j} - \hat{x}_{i,j}),
\]</div>
<p>and:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \hat{x}_{i,j}}{\partial z_{i,m}} = \sum_{k=1}^{3} w_{jk}' \cdot \hat{x}_{i,j}(1 - \hat{x}_{i,j}),
\]</div>
<p>the derivative of <span class="math notranslate nohighlight">\( z_{i,m} \)</span> with respect to <span class="math notranslate nohighlight">\( w_{mn} \)</span> is:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
\frac{\partial z_{i,m}}{\partial w_{mn}} = \sigma'(a_{i,m}) \cdot x_{i,n\\},
\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\( a_{i,m} = \sum_{n=1}^{3} w_{mn} x_{i,n} + b_m \)</span> and <span class="math notranslate nohighlight">\( \sigma'(a_{i,m}) = z_{i,m}(1 - z_{i,m}) \)</span>.</p>
<p>So:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}_{\text{MSE}}^{(i)}}{\partial w_{mn}} = -\sum_{j=1}^{3} (x_{i,j} - \hat{x}_{i,j}) \cdot \sum_{k=1}^{3} w_{jk}' \cdot \hat{x}_{i,j}(1 - \hat{x}_{i,j}) \cdot z_{i,m}(1 - z_{i,m}) \cdot x_{i,n}.
\]</div>
<p><em><strong>Gradient Descent</strong></em></p>
<p>Using the derived gradients, you can perform gradient descent to optimize the weights <span class="math notranslate nohighlight">\( W_e \)</span>, <span class="math notranslate nohighlight">\( W_d \)</span>, <span class="math notranslate nohighlight">\( \mathbf{b}_e \)</span>, and <span class="math notranslate nohighlight">\( \mathbf{b}_d \)</span> by updating them iteratively as follows:</p>
<div class="math notranslate nohighlight">
\[
W_e \leftarrow W_e - \eta \frac{\partial \mathcal{L}_{\text{MSE}}}{\partial W_e}, \quad W_d \leftarrow W_d - \eta \frac{\partial \mathcal{L}_{\text{MSE}}}{\partial W_d},
\]</div>
<p>where <span class="math notranslate nohighlight">\( \eta \)</span> is the learning rate.</p>
<p>This iterative process will help find the optimum weights that minimize the MSE loss.</p>
</section>
<section id="example-ae-ellipse-dataset-with-sigmoid-af">
<h4>Example AE : Ellipse dataset with sigmoid AF<a class="headerlink" href="#example-ae-ellipse-dataset-with-sigmoid-af" title="Link to this heading">#</a></h4>
<p>The autoencoder isn’t working well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Step 1: Generate a synthetic ellipse-shaped dataset</span>
<span class="k">def</span> <span class="nf">generate_ellipse_data</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">angles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
    <span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>  <span class="c1"># x = a * cos(theta)</span>
    <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>  <span class="c1"># y = b * sin(theta)</span>
    <span class="k">return</span> <span class="n">x_data</span>

<span class="n">M</span> <span class="o">=</span> <span class="mi">3000</span>  <span class="c1"># Number of samples</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span>  <span class="c1"># Semi-major and semi-minor axes of the ellipse</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">generate_ellipse_data</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># Step 2: Initialize Parameters</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Initialize weights and biases for encoder (W_e, b_e) and decoder (W_d, b_d)</span>
<span class="n">W_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
<span class="n">W_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span>

<span class="c1"># Sigmoid activation function and its derivative</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Step 3: Forward Pass</span>
<span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">):</span>
    <span class="c1"># Encoder</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_e</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_e</span><span class="p">)</span>
    
    <span class="c1"># Decoder</span>
    <span class="n">x_hat</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">W_d</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_d</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span>

<span class="c1"># Step 4: Compute Loss</span>
<span class="k">def</span> <span class="nf">compute_mse_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_hat</span><span class="p">))</span>

<span class="c1"># Step 5: Backward Pass</span>
<span class="k">def</span> <span class="nf">backward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="p">):</span>
    <span class="c1"># Calculate the gradient of the loss with respect to the output</span>
    <span class="n">d_loss</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_hat</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Gradients for the decoder</span>
    <span class="n">d_x_hat</span> <span class="o">=</span> <span class="n">d_loss</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">x_hat</span><span class="p">)</span>
    <span class="n">d_W_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_x_hat</span><span class="p">)</span>
    <span class="n">d_b_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d_x_hat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Gradients for the encoder</span>
    <span class="n">d_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d_x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">d_W_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_z</span><span class="p">)</span>
    <span class="n">d_b_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d_z</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span>

<span class="c1"># Step 6: Update Weights</span>
<span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">,</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">W_e</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_W_e</span>
    <span class="n">b_e</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_b_e</span>
    <span class="n">W_d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_W_d</span>
    <span class="n">b_d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_b_d</span>
    
    <span class="k">return</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span>

<span class="c1"># Step 7: Training Loop</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># Forward pass</span>
    <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">)</span>
    
    <span class="c1"># Compute loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_mse_loss</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">)</span>
    
    <span class="c1"># Backward pass</span>
    <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span> <span class="o">=</span> <span class="n">backward_pass</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="p">)</span>
    
    <span class="c1"># Update weights</span>
    <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">,</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
    
    <span class="c1"># Print the loss every 1000 epochs</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Step 8: Visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Original data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original Ellipse Data&#39;</span><span class="p">)</span>

<span class="c1"># Encoded data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Encoded&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Encoded Data&#39;</span><span class="p">)</span>

<span class="c1"># Decoded data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_hat</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hat</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decoded&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Decoded Data&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0, Loss: 4.748893752661329
Epoch 1000, Loss: 4.341809027893352
Epoch 2000, Loss: 4.115050082820742
Epoch 3000, Loss: 3.9530565714692676
Epoch 4000, Loss: 3.8477933534780435
Epoch 5000, Loss: 3.772675521668033
Epoch 6000, Loss: 3.693230782434496
Epoch 7000, Loss: 3.579149430152475
Epoch 8000, Loss: 3.4457780603760892
Epoch 9000, Loss: 3.3440150945964113
Epoch 10000, Loss: 3.279690401568481
Epoch 11000, Loss: 3.238175265976958
Epoch 12000, Loss: 3.2095569880008306
Epoch 13000, Loss: 3.1886725562577896
Epoch 14000, Loss: 3.1727567054487165
Epoch 15000, Loss: 3.1602201843306315
Epoch 16000, Loss: 3.1500882027329995
Epoch 17000, Loss: 3.141729716847396
Epoch 18000, Loss: 3.1347177428323474
Epoch 19000, Loss: 3.1287526526618694
</pre></div>
</div>
<img alt="../../_images/6639dc0f6488aa99634249f1f660a90cbdafa4e358ba460b9477cd463e55a161.png" src="../../_images/6639dc0f6488aa99634249f1f660a90cbdafa4e358ba460b9477cd463e55a161.png" />
</div>
</div>
</section>
<section id="example-ae-ellipse-dataset-with-relu-af">
<h4>Example AE : Ellipse dataset with RELU AF<a class="headerlink" href="#example-ae-ellipse-dataset-with-relu-af" title="Link to this heading">#</a></h4>
<p>Problem with mismatching AF and negative data for recovery.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="c1"># Step 1: Generate a synthetic ellipse-shaped dataset</span>
<span class="k">def</span> <span class="nf">generate_ellipse_data</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">angles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
    <span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span> <span class="c1"># x = a * cos(theta)</span>
    <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span> <span class="c1"># y = b * sin(theta)</span>
    <span class="k">return</span> <span class="n">x_data</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">3000</span> <span class="c1"># Number of samples</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span> <span class="c1"># Semi-major and semi-minor axes of the ellipse</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">generate_ellipse_data</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="c1"># Step 2: Initialize Parameters</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># Initialize weights and biases for encoder (W_e, b_e) and decoder (W_d, b_d)</span>
<span class="n">W_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
<span class="n">W_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span>
<span class="c1"># Leaky ReLU activation function and its derivative</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">leaky_relu_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
<span class="c1"># Step 3: Forward Pass</span>
<span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">):</span>
<span class="c1"># Encoder</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_e</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_e</span><span class="p">)</span>
<span class="c1"># Decoder</span>
    <span class="n">x_hat</span> <span class="o">=</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">W_d</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span>

<span class="c1"># Step 4: Compute Loss</span>
<span class="k">def</span> <span class="nf">compute_mse_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_hat</span><span class="p">))</span>
<span class="c1"># Step 5: Backward Pass</span>
<span class="k">def</span> <span class="nf">backward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="p">):</span>
    <span class="c1"># Calculate the gradient of the loss with respect to the output</span>
    <span class="n">d_loss</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_hat</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Gradients for the decoder</span>
    <span class="n">d_x_hat</span> <span class="o">=</span> <span class="n">d_loss</span> <span class="o">*</span> <span class="n">leaky_relu_derivative</span><span class="p">(</span><span class="n">x_hat</span><span class="p">)</span>
    <span class="n">d_W_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_x_hat</span><span class="p">)</span>
    <span class="n">d_b_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d_x_hat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Gradients for the encoder</span>
    <span class="n">d_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d_x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">leaky_relu_derivative</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">d_W_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_z</span><span class="p">)</span>
    <span class="n">d_b_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d_z</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span>
<span class="c1"># Step 6: Update Weights</span>
<span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">,</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">W_e</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_W_e</span>
    <span class="n">b_e</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_b_e</span>
    <span class="n">W_d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_W_d</span>
    <span class="n">b_d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_b_d</span>
    <span class="k">return</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span>
<span class="c1"># Step 7: Training Loop</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
<span class="c1"># Forward pass</span>
    <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">)</span>
    <span class="c1"># Compute loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_mse_loss</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">)</span>
    <span class="c1"># Backward pass</span>
    <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span> <span class="o">=</span> <span class="n">backward_pass</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="p">)</span>
    <span class="c1"># Update weights</span>
    <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">,</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
    <span class="c1"># Print the loss every 1000 epochs</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="c1"># Step 8: Visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="c1"># Original data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original Ellipse Data&#39;</span><span class="p">)</span>
<span class="c1"># Encoded data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Encoded&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Encoded Data&#39;</span><span class="p">)</span>
<span class="c1"># Decoded data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_hat</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hat</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decoded&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Decoded Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0, Loss: 4.499754268988179
Epoch 1000, Loss: 4.499993785287071
Epoch 2000, Loss: 4.499986048268678
Epoch 3000, Loss: 2.653328504901554
Epoch 4000, Loss: 2.2545405819444997
Epoch 5000, Loss: 2.2175216023302924
Epoch 6000, Loss: 2.209225350861131
Epoch 7000, Loss: 2.205853064223806
Epoch 8000, Loss: 2.2040865812047707
Epoch 9000, Loss: 2.2029080646285015
Epoch 10000, Loss: 2.2019588650491397
Epoch 11000, Loss: 2.2011152154221656
Epoch 12000, Loss: 2.2003370449250452
Epoch 13000, Loss: 2.199609757518492
Epoch 14000, Loss: 2.1989272753584532
Epoch 15000, Loss: 2.1982858099603164
Epoch 16000, Loss: 2.197680896714541
Epoch 17000, Loss: 2.197108819653527
Epoch 18000, Loss: 2.1965664939926848
Epoch 19000, Loss: 2.1960503758560272
</pre></div>
</div>
<img alt="../../_images/40cef397188122d137f351b958e73c5ba5f5a97d8d802d7860143f27f9e241f5.png" src="../../_images/40cef397188122d137f351b958e73c5ba5f5a97d8d802d7860143f27f9e241f5.png" />
</div>
</div>
</section>
<section id="example-ae-ellipse-dataset-with-relu-af-with-biasing-data">
<h4>Example AE : Ellipse dataset with RELU AF with Biasing data<a class="headerlink" href="#example-ae-ellipse-dataset-with-relu-af-with-biasing-data" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Step 1: Generate a synthetic ellipse-shaped dataset</span>
<span class="k">def</span> <span class="nf">generate_ellipse_data</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="n">angles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
    <span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>  <span class="c1"># x = a * cos(theta) + bias</span>
    <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>  <span class="c1"># y = b * sin(theta) + bias</span>
    <span class="k">return</span> <span class="n">x_data</span>

<span class="n">M</span> <span class="o">=</span> <span class="mi">3000</span>  <span class="c1"># Number of samples</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span>  <span class="c1"># Semi-major and semi-minor axes of the ellipse</span>
<span class="n">bias</span> <span class="o">=</span> <span class="mi">5</span>    <span class="c1"># Bias to shift data points to positive quadrant</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">generate_ellipse_data</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

<span class="c1"># Step 2: Initialize Parameters</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Initialize weights and biases for encoder (W_e, b_e) and decoder (W_d, b_d)</span>
<span class="n">W_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
<span class="n">W_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span>

<span class="c1"># Leaky ReLU activation function and its derivative</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">leaky_relu_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Step 3: Forward Pass</span>
<span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">):</span>
    <span class="c1"># Encoder</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_e</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_e</span><span class="p">)</span>
    
    <span class="c1"># Decoder</span>
    <span class="n">x_hat</span> <span class="o">=</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">W_d</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_d</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span>

<span class="c1"># Step 4: Compute Loss</span>
<span class="k">def</span> <span class="nf">compute_mse_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_hat</span><span class="p">))</span>

<span class="c1"># Step 5: Backward Pass</span>
<span class="k">def</span> <span class="nf">backward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="p">):</span>
    <span class="c1"># Calculate the gradient of the loss with respect to the output</span>
    <span class="n">d_loss</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_hat</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Gradients for the decoder</span>
    <span class="n">d_x_hat</span> <span class="o">=</span> <span class="n">d_loss</span> <span class="o">*</span> <span class="n">leaky_relu_derivative</span><span class="p">(</span><span class="n">x_hat</span><span class="p">)</span>
    <span class="n">d_W_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_x_hat</span><span class="p">)</span>
    <span class="n">d_b_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d_x_hat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Gradients for the encoder</span>
    <span class="n">d_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d_x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">leaky_relu_derivative</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">d_W_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_z</span><span class="p">)</span>
    <span class="n">d_b_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d_z</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span>

<span class="c1"># Step 6: Update Weights</span>
<span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">,</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">W_e</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_W_e</span>
    <span class="n">b_e</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_b_e</span>
    <span class="n">W_d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_W_d</span>
    <span class="n">b_d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_b_d</span>
    
    <span class="k">return</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span>

<span class="c1"># Step 7: Training Loop</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">25000</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.002</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># Forward pass</span>
    <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">)</span>
    
    <span class="c1"># Compute loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_mse_loss</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">)</span>
    
    <span class="c1"># Backward pass</span>
    <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span> <span class="o">=</span> <span class="n">backward_pass</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="p">)</span>
    
    <span class="c1"># Update weights</span>
    <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">,</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
    
    <span class="c1"># Print the loss every 1000 epochs</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Step 8: Visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Original data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original Ellipse Data&#39;</span><span class="p">)</span>

<span class="c1"># Encoded data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Encoded&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Encoded Data&#39;</span><span class="p">)</span>

<span class="c1"># Decoded data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_hat</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hat</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decoded&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Decoded Data&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0, Loss: 29.49941201881798
Epoch 1000, Loss: 2.2566424659306423
Epoch 2000, Loss: 2.2518089700498676
Epoch 3000, Loss: 2.2504704292750013
Epoch 4000, Loss: 2.2501143727819755
Epoch 5000, Loss: 2.25002120188349
Epoch 6000, Loss: 0.000580299663714989
Epoch 7000, Loss: 0.00014879959594686209
Epoch 8000, Loss: 4.1899492575914796e-05
Epoch 9000, Loss: 1.225127580580911e-05
Epoch 10000, Loss: 3.6855061058016317e-06
Epoch 11000, Loss: 1.1323733737986214e-06
Epoch 12000, Loss: 3.5366276636734785e-07
Epoch 13000, Loss: 1.1205068691484705e-07
Epoch 14000, Loss: 3.6041915985517185e-08
Epoch 15000, Loss: 1.1823218287497962e-08
Epoch 16000, Loss: 3.992076743616199e-09
Epoch 17000, Loss: 1.4088642495049594e-09
Epoch 18000, Loss: 5.316011810698347e-10
Epoch 19000, Loss: 2.2027373758305869e-10
Epoch 20000, Loss: 1.0221367661803239e-10
Epoch 21000, Loss: 5.326972625537228e-11
Epoch 22000, Loss: 3.07996851231567e-11
Epoch 23000, Loss: 1.9284238403450808e-11
Epoch 24000, Loss: 1.2726396995461276e-11
</pre></div>
</div>
<img alt="../../_images/cd1ecd7030916cf96d13dae2b3e89730b1dea0caee57544f01d4c056f51bdd28.png" src="../../_images/cd1ecd7030916cf96d13dae2b3e89730b1dea0caee57544f01d4c056f51bdd28.png" />
</div>
</div>
</section>
<section id="example-ae-regulated-sigmoid-for-hidden-and-relu-for-output">
<h4>Example AE : Regulated Sigmoid for hidden and RELU for output<a class="headerlink" href="#example-ae-regulated-sigmoid-for-hidden-and-relu-for-output" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Step 1: Generate a synthetic ellipse-shaped dataset</span>
<span class="k">def</span> <span class="nf">generate_ellipse_data</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="n">angles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
    <span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>  <span class="c1"># x = a * cos(theta) + bias</span>
    <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>  <span class="c1"># y = b * sin(theta) + bias</span>
    <span class="k">return</span> <span class="n">x_data</span>

<span class="n">M</span> <span class="o">=</span> <span class="mi">3000</span>  <span class="c1"># Number of samples</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span>  <span class="c1"># Semi-major and semi-minor axes of the ellipse</span>
<span class="n">bias</span> <span class="o">=</span> <span class="mi">4</span>    <span class="c1"># Bias to shift data points to positive quadrant</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">generate_ellipse_data</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

<span class="c1"># Step 2: Initialize Parameters</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Initialize weights and biases for encoder (W_e, b_e) and decoder (W_d, b_d)</span>
<span class="n">W_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
<span class="n">W_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span>

<span class="c1"># Sigmoid and ReLU activation functions and their derivatives</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">sig</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sig</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sig</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">relu_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 3: Forward Pass</span>
<span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">):</span>
    <span class="c1"># Encoder</span>
    <span class="n">z</span> <span class="o">=</span><span class="mi">3</span><span class="o">*</span> <span class="n">sigmoid</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_e</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_e</span><span class="p">)</span>
    
    <span class="c1"># Decoder</span>
    <span class="n">x_hat</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">W_d</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_d</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span>

<span class="c1"># Step 4: Compute Loss</span>
<span class="k">def</span> <span class="nf">compute_mse_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_hat</span><span class="p">))</span>

<span class="c1"># Step 5: Backward Pass</span>
<span class="k">def</span> <span class="nf">backward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="p">):</span>
    <span class="c1"># Calculate the gradient of the loss with respect to the output</span>
    <span class="n">d_loss</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_hat</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Gradients for the decoder</span>
    <span class="n">d_x_hat</span> <span class="o">=</span> <span class="n">d_loss</span> <span class="o">*</span> <span class="n">relu_derivative</span><span class="p">(</span><span class="n">x_hat</span><span class="p">)</span>
    <span class="n">d_W_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_x_hat</span><span class="p">)</span>
    <span class="n">d_b_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d_x_hat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Gradients for the encoder</span>
    <span class="n">d_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d_x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">d_W_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_z</span><span class="p">)</span>
    <span class="n">d_b_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d_z</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span>

<span class="c1"># Step 6: Update Weights</span>
<span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">,</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">W_e</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_W_e</span>
    <span class="n">b_e</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_b_e</span>
    <span class="n">W_d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_W_d</span>
    <span class="n">b_d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_b_d</span>
    
    <span class="k">return</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span>

<span class="c1"># Step 7: Training Loop</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># Forward pass</span>
    <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">)</span>
    
    <span class="c1"># Compute loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_mse_loss</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">)</span>
    
    <span class="c1"># Backward pass</span>
    <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span> <span class="o">=</span> <span class="n">backward_pass</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="p">)</span>
    
    <span class="c1"># Update weights</span>
    <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">,</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
    
    <span class="c1"># Print the loss every 1000 epochs</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Step 8: Visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Original data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original Ellipse Data&#39;</span><span class="p">)</span>

<span class="c1"># Encoded data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Encoded&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Encoded Data&#39;</span><span class="p">)</span>

<span class="c1"># Decoded data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_hat</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hat</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decoded&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Decoded Data&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0, Loss: 20.252265033373288
Epoch 1000, Loss: 2.840976348160917
Epoch 2000, Loss: 0.11302243902716794
Epoch 3000, Loss: 0.01273963803882825
Epoch 4000, Loss: 0.010501905251744159
Epoch 5000, Loss: 0.009174833832633712
Epoch 6000, Loss: 0.008199820269915533
Epoch 7000, Loss: 0.007724960287272952
Epoch 8000, Loss: 0.007615318199105559
Epoch 9000, Loss: 0.007710862445879562
Epoch 10000, Loss: 0.007886537922729548
Epoch 11000, Loss: 0.008060981853574484
Epoch 12000, Loss: 0.008189929939366477
Epoch 13000, Loss: 0.008255497141464126
Epoch 14000, Loss: 0.008256180657243773
Epoch 15000, Loss: 0.008199297034555833
Epoch 16000, Loss: 0.008095942844869851
Epoch 17000, Loss: 0.007957958846096782
Epoch 18000, Loss: 0.00779629043372439
Epoch 19000, Loss: 0.0076202425251320465
</pre></div>
</div>
<img alt="../../_images/8191077656842a0c1cef7491b6f740802891346ee82b06fe85a9dc728e6620ff.png" src="../../_images/8191077656842a0c1cef7491b6f740802891346ee82b06fe85a9dc728e6620ff.png" />
</div>
</div>
</section>
<section id="conclusion">
<h4>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h4>
<p>Understanding the impact of activation functions and matching them with the dataset can significantly influence the performance of neural networks, including autoencoders.</p>
<p><strong>Matching Activation Functions with Datasets:</strong></p>
<p>Choose activation functions based on dataset characteristics; for data in a specific range like 0-1, sigmoid is suitable, while ReLU or its variants are better for broader ranges. For reconstruction tasks, align output activations with the data distribution, and use sigmoid for binary classification, ReLU or linear functions for regression.</p>
<p><strong>Understanding Data Exceptions:</strong></p>
<p>Account for bias shifts and normalization in data by choosing activation functions that complement these transformations. Address overfitting with regularization and underfitting by adjusting model complexity and activation functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Step 1: Generate a synthetic ellipse-shaped dataset</span>
<span class="k">def</span> <span class="nf">generate_ellipse_data</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">noise_factor</span><span class="p">):</span>
    <span class="n">angles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
    <span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>  <span class="c1"># x = a * cos(theta) + bias</span>
    <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>  <span class="c1"># y = b * sin(theta) + bias</span>
    <span class="c1"># Add Gaussian noise</span>
    <span class="n">x_data</span> <span class="o">+=</span> <span class="n">noise_factor</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_data</span>

<span class="c1"># Parameters</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">3000</span>  <span class="c1"># Number of samples</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span>  <span class="c1"># Semi-major and semi-minor axes of the ellipse</span>
<span class="n">bias</span> <span class="o">=</span> <span class="mi">10</span>    <span class="c1"># Bias to shift data points to positive quadrant</span>

<span class="c1"># Generate training data with little noise</span>
<span class="n">train_noise_factor</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">generate_ellipse_data</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">train_noise_factor</span><span class="p">)</span>

<span class="c1"># Generate testing data with more noise</span>
<span class="n">test_noise_factor</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">generate_ellipse_data</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">test_noise_factor</span><span class="p">)</span>

<span class="c1"># Step 2: Initialize Parameters</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Initialize weights and biases for encoder (W_e, b_e) and decoder (W_d, b_d)</span>
<span class="n">W_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
<span class="n">W_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span>

<span class="c1"># Leaky ReLU activation function and its derivative</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">leaky_relu_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Step 3: Forward Pass</span>
<span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">):</span>
    <span class="c1"># Encoder</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W_e</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_e</span><span class="p">)</span>
    
    <span class="c1"># Decoder</span>
    <span class="n">x_hat</span> <span class="o">=</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">W_d</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_d</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span>

<span class="c1"># Step 4: Compute Loss</span>
<span class="k">def</span> <span class="nf">compute_mse_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_hat</span><span class="p">))</span>

<span class="c1"># Step 5: Backward Pass</span>
<span class="k">def</span> <span class="nf">backward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="p">):</span>
    <span class="c1"># Calculate the gradient of the loss with respect to the output</span>
    <span class="n">d_loss</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_hat</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Gradients for the decoder</span>
    <span class="n">d_x_hat</span> <span class="o">=</span> <span class="n">d_loss</span> <span class="o">*</span> <span class="n">leaky_relu_derivative</span><span class="p">(</span><span class="n">x_hat</span><span class="p">)</span>
    <span class="n">d_W_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_x_hat</span><span class="p">)</span>
    <span class="n">d_b_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d_x_hat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Gradients for the encoder</span>
    <span class="n">d_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d_x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">leaky_relu_derivative</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">d_W_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_z</span><span class="p">)</span>
    <span class="n">d_b_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d_z</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span>

<span class="c1"># Step 6: Update Weights</span>
<span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">,</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">W_e</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_W_e</span>
    <span class="n">b_e</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_b_e</span>
    <span class="n">W_d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_W_d</span>
    <span class="n">b_d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d_b_d</span>
    
    <span class="k">return</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span>

<span class="c1"># Step 7: Training Loop</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">25000</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.002</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># Forward pass</span>
    <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">)</span>
    
    <span class="c1"># Compute loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_mse_loss</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">)</span>
    
    <span class="c1"># Backward pass</span>
    <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span> <span class="o">=</span> <span class="n">backward_pass</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">W_d</span><span class="p">)</span>
    
    <span class="c1"># Update weights</span>
    <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">,</span> <span class="n">d_W_e</span><span class="p">,</span> <span class="n">d_b_e</span><span class="p">,</span> <span class="n">d_W_d</span><span class="p">,</span> <span class="n">d_b_d</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
    
    <span class="c1"># Print the loss every 1000 epochs</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Step 8: Test with noisy data</span>
<span class="n">z_test</span><span class="p">,</span> <span class="n">x_hat_test</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">W_e</span><span class="p">,</span> <span class="n">b_e</span><span class="p">,</span> <span class="n">W_d</span><span class="p">,</span> <span class="n">b_d</span><span class="p">)</span>

<span class="c1"># Step 9: Visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Noisy training data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Data (Noisy)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training Data with Little Noise&#39;</span><span class="p">)</span>

<span class="c1"># Encoded testing data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Encoded Testing Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Encoded Data from Noisy Test Data&#39;</span><span class="p">)</span>

<span class="c1"># Decoded testing data (2D)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_hat_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_hat_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Reconstructed Testing Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Reconstructed Data from Noisy Test Data&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0, Loss: 104.46953078746617
Epoch 1000, Loss: 2.2527683354006762
Epoch 2000, Loss: 2.252203762479861
Epoch 3000, Loss: 2.251801339063604
Epoch 4000, Loss: 2.251513832863968
Epoch 5000, Loss: 2.251307281015377
Epoch 6000, Loss: 2.219941603358059
Epoch 7000, Loss: 0.0002657280160549213
Epoch 8000, Loss: 0.00018718653299095678
Epoch 9000, Loss: 0.00013350008686126584
Epoch 10000, Loss: 9.542292842007974e-05
Epoch 11000, Loss: 6.823646728911043e-05
Epoch 12000, Loss: 4.880544660796574e-05
Epoch 13000, Loss: 3.4913997144378694e-05
Epoch 14000, Loss: 2.4981147675858338e-05
Epoch 15000, Loss: 1.787783706902638e-05
Epoch 16000, Loss: 1.2797337125210799e-05
Epoch 17000, Loss: 9.163087070224503e-06
Epoch 18000, Loss: 6.562972873748675e-06
Epoch 19000, Loss: 4.702418398114842e-06
Epoch 20000, Loss: 3.3709005888988313e-06
Epoch 21000, Loss: 2.4174956835894043e-06
Epoch 22000, Loss: 1.7346519868753245e-06
Epoch 23000, Loss: 1.2454563569695877e-06
Epoch 24000, Loss: 8.948935362657074e-07
</pre></div>
</div>
<img alt="../../_images/35f99565fadadd3e798763de23ac3a97dfa79ed3a10b866904386ffe1725ba0f.png" src="../../_images/35f99565fadadd3e798763de23ac3a97dfa79ed3a10b866904386ffe1725ba0f.png" />
</div>
</div>
</section>
</section>
</section>
<section id="homework-extend-the-previous-work-with-a-3d-sphere">
<h2>Homework: Extend the Previous Work with a 3D Sphere<a class="headerlink" href="#homework-extend-the-previous-work-with-a-3d-sphere" title="Link to this heading">#</a></h2>
<p><strong>Objective:</strong> Continue from the previous exercise, now applying the concepts to a 3-dimensional sphere.</p>
<p><strong>Tasks:</strong></p>
<p><strong>Data Generation:</strong></p>
<ul class="simple">
<li><p>Generate a 3D sphere dataset.</p></li>
<li><p>Ensure the data has 3 dimensions for both input and output, with a 2-dimensional hidden layer.</p></li>
</ul>
<p><strong>Denoising Application:</strong></p>
<ul class="simple">
<li><p>Apply the autoencoder to denoise the 3D sphere data.</p></li>
<li><p>Add noise to the input data and train the autoencoder to reconstruct the original, noise-free data.</p></li>
</ul>
<p><strong>Parameter Tuning:</strong></p>
<ul class="simple">
<li><p>Experiment with different parameters such as learning rate, number of epochs, and noise levels.</p></li>
<li><p>Observe how these changes affect the performance of the autoencoder in terms of reconstruction quality.</p></li>
</ul>
<p><strong>Discussion on Fundamentals:</strong></p>
<ul class="simple">
<li><p>Discuss the role and significance of the hidden layer in the autoencoder.</p></li>
<li><p>Explore how the 2-dimensional hidden layer encodes the 3-dimensional input and what it represents.</p></li>
</ul>
</section>
<section id="miniproject-complete-ae-with-binary-cross-entropy-bce">
<h2>Miniproject complete AE with Binary Cross-Entropy (BCE)<a class="headerlink" href="#miniproject-complete-ae-with-binary-cross-entropy-bce" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Text and mathematics</p></li>
<li><p>Example</p></li>
</ul>
<hr class="docutils" />
<section id="convolutional-autoencoders">
<h3><strong>Convolutional Autoencoders</strong>:<a class="headerlink" href="#convolutional-autoencoders" title="Link to this heading">#</a></h3>
<p>Use convolutional layers instead of fully connected layers, making them well-suited for image data. They can capture spatial hierarchies and local patterns.</p>
<section id="convolutional-autoencoders-cae">
<h4>Convolutional Autoencoders (CAE)<a class="headerlink" href="#convolutional-autoencoders-cae" title="Link to this heading">#</a></h4>
<p>Convolutional Autoencoders (CAE) are a type of autoencoder where the encoder and decoder are made using convolutional layers, making them particularly suited for image data. They are powerful tools for tasks like image denoising, image compression, and feature extraction.</p>
</section>
<section id="structure-of-a-convolutional-autoencoder">
<h4>Structure of a Convolutional Autoencoder<a class="headerlink" href="#structure-of-a-convolutional-autoencoder" title="Link to this heading">#</a></h4>
<p>A typical Convolutional Autoencoder consists of two parts:</p>
<p>Encoder: Compresses the input image into a lower-dimensional latent space. This is achieved using convolutional and pooling layers.
Decoder: Reconstructs the image from the latent space back to the original dimensions. This is done using upsampling and convolutional layers.</p>
<p><em><strong>Example Noisy Images</strong></em></p>
<p>Explanation:
Dataset: MNIST is used, which consists of 28x28 grayscale images of handwritten digits. The images are normalized to [0, 1] and reshaped to include a channel dimension.
Encoder:
Conv2D: Applies convolutional filters to detect features like edges, corners, etc.
MaxPooling2D: Reduces the spatial dimensions, creating a compressed representation.
Latent Space: This is the compressed form of the image, typically of much lower dimensionality.
Decoder:
UpSampling2D: Increases the spatial dimensions to reconstruct the image.
Conv2D: Applies convolutional filters to reconstruct the image from the latent space.
Training: The model is trained to minimize the difference between the original and reconstructed images using binary crossentropy loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span><span class="p">,</span> <span class="n">UpSampling2D</span><span class="p">,</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Load the MNIST dataset from a local .npz file</span>
<span class="n">mnist_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;mnist.npz&#39;</span><span class="p">)</span>

<span class="c1"># Extract the training and test sets</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">mnist_data</span><span class="p">[</span><span class="s1">&#39;x_train&#39;</span><span class="p">]</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">mnist_data</span><span class="p">[</span><span class="s1">&#39;x_test&#39;</span><span class="p">]</span>

<span class="c1"># Normalize and reshape the data to fit the model (adding a channel dimension)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">input_shape</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

<span class="c1"># Add random noise to the images</span>
<span class="n">noise_factor</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">x_train_noisy</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">+</span> <span class="n">noise_factor</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">x_test_noisy</span> <span class="o">=</span> <span class="n">x_test</span> <span class="o">+</span> <span class="n">noise_factor</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Clip the values to be in the range [0, 1]</span>
<span class="n">x_train_noisy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x_train_noisy</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">x_test_noisy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Build the Encoder</span>
<span class="n">input_img</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">input_img</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Latent space</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Build the Decoder</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">encoded</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">UpSampling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">UpSampling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="n">decoded</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Define the autoencoder model</span>
<span class="n">autoencoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_img</span><span class="p">,</span> <span class="n">decoded</span><span class="p">)</span>

<span class="c1"># Compile the model</span>
<span class="n">autoencoder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">)</span>

<span class="c1"># Train the autoencoder</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">autoencoder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_noisy</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">,</span> <span class="n">x_test</span><span class="p">))</span>

<span class="c1"># Encode and decode some digits from the test set</span>
<span class="n">decoded_imgs</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">)</span>

<span class="c1"># Display original noisy and reconstructed images</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of images to display</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="c1"># Display original noisy image</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># Display reconstruction</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">decoded_imgs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

    <span class="c1"># Display original clean image for comparison</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">469/469</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">87s</span> 181ms/step - loss: 0.2062 - val_loss: 0.1046
<span class=" -Color -Color-Bold">313/313</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">5s</span> 16ms/step
</pre></div>
</div>
<img alt="../../_images/37fa39ba83fe2b0c48b66a41adebee1de67364462182080dcc49592048ed4344.png" src="../../_images/37fa39ba83fe2b0c48b66a41adebee1de67364462182080dcc49592048ed4344.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="c1"># Print the summary of the autoencoder to identify the correct layer</span>
<span class="n">autoencoder</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="c1"># Identify the index or name of the layer you want to visualize</span>
<span class="n">conv_layer</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># Update the index if necessary</span>

<span class="c1"># Get the weights of the convolutional layer</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="c1"># Check if the weights are available</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">filters</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="n">weights</span>
    
    <span class="c1"># Normalize filter values between 0 and 1 for visualization</span>
    <span class="n">f_min</span><span class="p">,</span> <span class="n">f_max</span> <span class="o">=</span> <span class="n">filters</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">filters</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">filters</span> <span class="o">=</span> <span class="p">(</span><span class="n">filters</span> <span class="o">-</span> <span class="n">f_min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">f_max</span> <span class="o">-</span> <span class="n">f_min</span><span class="p">)</span>

    <span class="n">n_filters</span> <span class="o">=</span> <span class="n">filters</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Number of filters</span>

    <span class="c1"># Plot the filters</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_filters</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">filters</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The selected layer does not have any trainable weights.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_7"</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_7 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">28</span>, <span style="color: #00af00; text-decoration-color: #00af00">28</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_18 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)              │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">28</span>, <span style="color: #00af00; text-decoration-color: #00af00">28</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)     │           <span style="color: #00af00; text-decoration-color: #00af00">320</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_6 (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>)  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)     │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_19 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)              │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │        <span style="color: #00af00; text-decoration-color: #00af00">18,496</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_7 (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>)  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)       │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_20 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)              │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)      │        <span style="color: #00af00; text-decoration-color: #00af00">73,856</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_21 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)              │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)      │       <span style="color: #00af00; text-decoration-color: #00af00">147,584</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling2d_6 (<span style="color: #0087ff; text-decoration-color: #0087ff">UpSampling2D</span>)  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_22 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)              │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">14</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │        <span style="color: #00af00; text-decoration-color: #00af00">73,792</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling2d_7 (<span style="color: #0087ff; text-decoration-color: #0087ff">UpSampling2D</span>)  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">28</span>, <span style="color: #00af00; text-decoration-color: #00af00">28</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_23 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)              │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">28</span>, <span style="color: #00af00; text-decoration-color: #00af00">28</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)      │           <span style="color: #00af00; text-decoration-color: #00af00">577</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">943,877</span> (3.60 MB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">314,625</span> (1.20 MB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Optimizer params: </span><span style="color: #00af00; text-decoration-color: #00af00">629,252</span> (2.40 MB)
</pre>
</div><img alt="../../_images/86fffe82dc0a7ef9a78dd2d548369d6f4988ff735425006e0cc866a7b3c6e9f0.png" src="../../_images/86fffe82dc0a7ef9a78dd2d548369d6f4988ff735425006e0cc866a7b3c6e9f0.png" />
</div>
</div>
</section>
</section>
</section>
<section id="miniproject-conv-ae">
<h2>Miniproject: Conv AE<a class="headerlink" href="#miniproject-conv-ae" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>complete with code</p></li>
<li><p>concept of pooling, upsampling, filter</p></li>
</ul>
</section>
<section id="miniproject-1-2-3-conv-vae">
<h2>Miniproject 1, 2 , 3: Conv VAE<a class="headerlink" href="#miniproject-1-2-3-conv-vae" title="Link to this heading">#</a></h2>
<p><strong>Variational Autoencoders (VAEs)</strong>: Instead of learning a deterministic latent representation, VAEs learn a probabilistic representation. This makes them suitable for generating new data samples.</p>
<p><strong>Data Generation</strong> : Variational autoencoders can generate new data samples similar to the training data, useful in tasks like image synthesis and data augmentation.</p>
<p><strong>Feature Learning</strong> : Autoencoders can learn useful features from the data that can be used in other machine learning tasks.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ML\FeatureReduction"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="PCA.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Principal Component Analysis</p>
      </div>
    </a>
    <a class="right-next"
       href="LLE.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Locally Linear Embedding (LLE)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoders-brief">Autoencoders Brief</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#structure-of-autoencoders">Structure of Autoencoders</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ae-formulation">AE formulation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-architecture">Define Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#guide-for-showing-architecture">Guide for Showing Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-features-in-handwritten-digits">Latent features in handwritten digits</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-denoising-mnist-using-ae">Example : Denoising MNIST using AE</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-only">Test Only</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#denoising-with-ae">Denoising with AE</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-algorithm-for-denoising-images-using-an-autoencoder">General Algorithm for Denoising Images Using an Autoencoder</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-autoencoders">Types of Autoencoders</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vanilla-autoencoders"><strong>Vanilla Autoencoders</strong>:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-function-of-the-output-layer">Activation Function of the Output Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-of-f-xi-in-activation-functions">Importance of <span class="math notranslate nohighlight">\( f'(\xi)\)</span> in Activation Functions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#categorization-of-activation-functions-based-on-f-xi">Categorization of Activation Functions Based on <span class="math notranslate nohighlight">\( f'(\xi)\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions-from-perspective-of-derivatives-as-xi">Activation Functions from Perspective of derivatives as <span class="math notranslate nohighlight">\(\xi \)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-scalar-concave-activation-functions-satisfying-lim-xi-to-infty-sigma-xi-0"><em>Continuous Scalar Concave Activation Functions Satisfying <span class="math notranslate nohighlight">\(\lim_{\xi \to \infty} \sigma'(\xi) = 0\)</span></em></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-scalar-concave-activation-function-satisfying-lim-xi-to-infty-sigma-xi-1"><em>Continuous Scalar Concave Activation Function Satisfying <span class="math notranslate nohighlight">\(\lim_{\xi \to \infty} \sigma'(\xi) = 1\)</span></em></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-loss-function-in-autoencoders">Understanding the Loss Function in Autoencoders</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-ae-ellipse-dataset-with-sigmoid-af">Example AE : Ellipse dataset with sigmoid AF</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-ae-ellipse-dataset-with-relu-af">Example AE : Ellipse dataset with RELU AF</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-ae-ellipse-dataset-with-relu-af-with-biasing-data">Example AE : Ellipse dataset with RELU AF with Biasing data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-ae-regulated-sigmoid-for-hidden-and-relu-for-output">Example AE : Regulated Sigmoid for hidden and RELU for output</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#homework-extend-the-previous-work-with-a-3d-sphere">Homework: Extend the Previous Work with a 3D Sphere</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#miniproject-complete-ae-with-binary-cross-entropy-bce">Miniproject complete AE with Binary Cross-Entropy (BCE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-autoencoders"><strong>Convolutional Autoencoders</strong>:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-autoencoders-cae">Convolutional Autoencoders (CAE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#structure-of-a-convolutional-autoencoder">Structure of a Convolutional Autoencoder</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#miniproject-conv-ae">Miniproject: Conv AE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#miniproject-1-2-3-conv-vae">Miniproject 1, 2 , 3: Conv VAE</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr.Hadi Sadoghi Yazdi
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024 Pattern Recognition Lab.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>