{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainable Combiners\n",
    "\n",
    "To begin, we start with two classes to understand the mean and covariance of their errors. We treat each as a two-dimensional feature generated for each sample error, so we have *n* samples of two-dimensional errors.\n",
    "\n",
    "**TABLE 5.1 Hypothetical Output for $ \\omega_1 $ from Two Classifiers for a Data Set $ Z = \\{z_1, \\dots, z_{10}\\} $.**\n",
    "\n",
    "| Data Point | $ z_1 $ | $ z_2 $ | $ z_3 $ | $ z_4 $ | $ z_5 $ | $ z_6 $ | $ z_7 $ | $ z_8 $ | $ z_9 $ | $ z_{10} $ |\n",
    "|------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-------------|\n",
    "| $ d_{1,1}(z_k) $ | 0.71 | 0.41 | 0.76 | 0.27 | 0.15 | 0.91 | 0.09 | 0.15 | 0.15 | 0.64 |\n",
    "| $ d_{2,1}(z_k) $ | 0.62 | 0.90 | 0.98 | 0.68 | 0.56 | 0.95 | 0.44 | 0.22 | 0.79 | 0.14 |\n",
    "| Target ($ \\omega_1 $) | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "The approximation error of $ D_1 $ has the following 10 values:\n",
    "\n",
    "$$\n",
    "(1 - 0.71), \\, (1 - 0.76), \\, (1 - 0.15), \\, 0.09, \\, 0.15, \\, 0.62, \\, 0.98, \\, 0.56, \\, 0.44, \\, 0.79\n",
    "$$\n",
    "\n",
    "The mean of the error of $D_1$ is $0.225$, and the variance of the error of $D_1$ is calculated as follows:\n",
    "\n",
    "$$\n",
    "s_1^2 = \\frac{1}{10 - 1} \\left( \\left(1 - 0.71 + 0.225\\right)^2 + \\left(1 - 0.76 + 0.225\\right)^2 + \\dots + \\left(0.79 + 0.225\\right)^2 \\right) \\approx 0.32\n",
    "$$\n",
    "\n",
    "The covariance matrix of the approximation errors of the two classifiers is:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{pmatrix} \n",
    "0.32 & 0.22 \\\\\n",
    "0.22 & 0.30 \n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What means the $ \\Sigma $-Norm\n",
    "\n",
    "Minimizing the $ \\Sigma $-norm of the weights in a classifier ensemble is a strategic approach to achieve an optimal combination of classifiers that takes into account the dependencies between their errors with more emphasis over robust clasifier. \n",
    "\n",
    "### Understanding the Problem\n",
    "\n",
    "In an ensemble of classifiers, the final decision is typically made by a weighted combination of the outputs of individual classifiers. Let:\n",
    "- $ w = [w_1, w_2, \\dots, w_L]^T $ represent the weights assigned to each of L-classifier.\n",
    "- $ \\Sigma $ be the covariance matrix of the errors made by the individual classifiers.\n",
    "\n",
    "The goal is to find the optimal set of weights $ w $ that combine the classifiers for finding support for each column of $ DP(x) $.\n",
    "\n",
    "### The Role of the $ \\Sigma $-Norm\n",
    "\n",
    "The $ \\Sigma $-norm $ \\|w\\|_\\Sigma $ is defined as:\n",
    "\n",
    "$$\n",
    "\\|w\\|_\\Sigma = \\sqrt{w^T \\Sigma w}\n",
    "$$\n",
    "\n",
    "This expression represents a weighted combination of the variances and covariances of the classifier errors, which can be interpreted as a measure of the overall **uncertainty** or risk associated with the combined classifier decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Problem\n",
    "After obtaining $\\Sigma$ of error, for applying to each column of DP, we want to minimize the $ \\Sigma $-norm $ \\|w\\|_\\Sigma $, defined as:\n",
    "\n",
    "$$\n",
    "\\|w\\|_\\Sigma = \\sqrt{w^T \\Sigma w}\n",
    "$$\n",
    "subject to the constraint:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{M} w_i = 1\n",
    "$$\n",
    "\n",
    "### Optimization Problem\n",
    "The problem can be formulated as the following constrained optimization:\n",
    "\n",
    "$$\n",
    "\\min_w \\quad \\sqrt{w^T \\Sigma w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{subject to} \\quad \\sum_{i=1}^{M} w_i = 1\n",
    "$$\n",
    "\n",
    "### Lagrangian Formulation\n",
    "Introduce a Lagrange multiplier $ \\lambda $ for the equality constraint:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w, \\lambda) = \\sqrt{w^T \\Sigma w} + \\lambda \\left(\\sum_{i=1}^{M} w_i - 1\\right)\n",
    "$$\n",
    "\n",
    "### Solve for $ w $ and $ \\lambda $\n",
    "\n",
    "#### Simplify the Objective Function\n",
    "Since $ \\|w\\|_\\Sigma = \\sqrt{w^T \\Sigma w} $, minimize $ w^T \\Sigma w $ instead to simplify the calculations.\n",
    "\n",
    "The problem becomes:\n",
    "\n",
    "$$\n",
    "\\min_w \\quad w^T \\Sigma w\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{subject to} \\quad \\sum_{i=1}^{M} w_i = 1\n",
    "$$\n",
    "\n",
    "#### Set Up the Lagrangian\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w, \\lambda) = w^T \\Sigma w + \\lambda \\left(\\sum_{i=1}^{M} w_i - 1\\right)\n",
    "$$\n",
    "\n",
    "#### Take Derivatives\n",
    "To find the minimum, take the derivative of the Lagrangian with respect to $ w $ and $ \\lambda $ and set them to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w} = 2\\Sigma w + \\lambda \\mathbf{1} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = \\sum_{i=1}^{M} w_i - 1 = 0\n",
    "$$\n",
    "\n",
    "#### Solve for $ w $\n",
    "From the first derivative equation:\n",
    "\n",
    "$$\n",
    "2\\Sigma w = -\\lambda \\mathbf{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = -\\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1}\n",
    "$$\n",
    "\n",
    "Using the constraint $ \\sum_{i=1}^{M} w_i = 1 $:\n",
    "\n",
    "$$\n",
    "-\\frac{\\lambda}{2} \\mathbf{1}^T \\Sigma^{-1} \\mathbf{1} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda = -\\frac{2}{\\mathbf{1}^T \\Sigma^{-1} \\mathbf{1}}\n",
    "$$\n",
    "\n",
    "Substitute $ \\lambda $ back into the expression for $ w $:\n",
    "\n",
    "$$\n",
    "w = \\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^T \\Sigma^{-1} \\mathbf{1}}\n",
    "$$\n",
    "\n",
    "### Solution\n",
    "The optimal weights $ w $ that minimize the $ \\Sigma $-norm under the constraint that their sum equals one are given by:\n",
    "\n",
    "$$\n",
    "w^* = \\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^T \\Sigma^{-1} \\mathbf{1}}\n",
    "$$\n",
    "\n",
    "### Interpretation\n",
    "- $ \\Sigma^{-1} $ is the inverse of the covariance matrix of errors.\n",
    "- The resulting weights $ w^* $ adjust according to the error covariance structure, minimizing the $ \\Sigma $-norm and effectively combining the classifiers while considering their error dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sopecial case: If the covariance matrix $ \\Sigma $ is diagonal\n",
    "\n",
    "$$\n",
    "\\Sigma = \\text{diag}(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_M^2)\n",
    "$$\n",
    "\n",
    "where $ \\sigma_i^2 $ is the variance associated with the $i$-th classifier.\n",
    "\n",
    "### Compute $ \\Sigma^{-1} $\n",
    "\n",
    "For a diagonal matrix $ \\Sigma $, the inverse $ \\Sigma^{-1} $ is also a diagonal matrix with entries being the reciprocals of the diagonal entries of $ \\Sigma $:\n",
    "\n",
    "$$\n",
    "\\Sigma^{-1} = \\text{diag}\\left(\\frac{1}{\\sigma_1^2}, \\frac{1}{\\sigma_2^2}, \\dots, \\frac{1}{\\sigma_M^2}\\right)\n",
    "$$\n",
    "\n",
    "### Compute $ \\mathbf{1}^T \\Sigma^{-1} \\mathbf{1} $\n",
    "\n",
    "Here, $ \\mathbf{1} $ is a vector of ones:\n",
    "\n",
    "$$\n",
    "\\mathbf{1} = [1, 1, \\dots, 1]^T\n",
    "$$\n",
    "\n",
    "To compute $ \\mathbf{1}^T \\Sigma^{-1} \\mathbf{1} $, you first calculate $ \\Sigma^{-1} \\mathbf{1} $:\n",
    "\n",
    "$$\n",
    "\\Sigma^{-1} \\mathbf{1} = \\left[\\frac{1}{\\sigma_1^2}, \\frac{1}{\\sigma_2^2}, \\dots, \\frac{1}{\\sigma_M^2}\\right]^T\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\mathbf{1}^T \\Sigma^{-1} \\mathbf{1} = \\sum_{i=1}^{M} \\frac{1}{\\sigma_i^2}\n",
    "$$\n",
    "\n",
    "### Compute $ w^* $\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "w^* = \\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^T \\Sigma^{-1} \\mathbf{1}}\n",
    "$$\n",
    "\n",
    "Substitute $ \\Sigma^{-1} \\mathbf{1} $ and $ \\mathbf{1}^T \\Sigma^{-1} \\mathbf{1} $:\n",
    "\n",
    "$$\n",
    "\\Sigma^{-1} \\mathbf{1} = \\left[\\frac{1}{\\sigma_1^2}, \\frac{1}{\\sigma_2^2}, \\dots, \\frac{1}{\\sigma_M^2}\\right]^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{1}^T \\Sigma^{-1} \\mathbf{1} = \\sum_{i=1}^{M} \\frac{1}{\\sigma_i^2}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "w^* = \\frac{\\left[\\frac{1}{\\sigma_1^2}, \\frac{1}{\\sigma_2^2}, \\dots, \\frac{1}{\\sigma_M^2}\\right]^T}{\\sum_{i=1}^{M} \\frac{1}{\\sigma_i^2}}\n",
    "$$\n",
    "\n",
    "### Result\n",
    "\n",
    "The optimal weights $ w^* $ are given by:\n",
    "\n",
    "$$\n",
    "w_i^* = \\frac{\\frac{1}{\\sigma_i^2}}{\\sum_{j=1}^{M} \\frac{1}{\\sigma_j^2}}\n",
    "$$\n",
    "\n",
    "where $ w_i^* $ is the weight assigned to the $ i $-th classifier. This result reflects that classifiers with smaller variances (i.e., more reliable classifiers) are assigned higher weights in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Entropy orness and $ \\Sigma $-norm\n",
    "\n",
    "To solve the optimization problem using gradient descent, follow these steps:\n",
    "\n",
    "### Problem Restatement\n",
    "\n",
    "You need to minimize the function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) = w^T \\Sigma w - \\nu \\sum_{i=1}^{L} w_i \\ln(w_i)\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{L} w_i = 1\n",
    "$$\n",
    "\n",
    "Here, $ w $ is the vector of weights, $ \\Sigma $ is a diagonal covariance matrix, and $ \\nu $ is a Lagrange multiplier associated with the entropy term.\n",
    "\n",
    "### Gradient Descent Solution\n",
    "\n",
    "**Define the Objective Function**\n",
    "\n",
    "The objective function to minimize is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) = w^T \\Sigma w - \\nu \\sum_{i=1}^{L} w_i \\ln(w_i)\n",
    "$$\n",
    "\n",
    "**Compute the Gradient**\n",
    "\n",
    "Calculate the gradient of $ \\mathcal{L} $ with respect to $ w_i $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_i} = 2 (\\Sigma w)_i - \\nu (\\ln w_i + 1)\n",
    "$$\n",
    "\n",
    "where $ (\\Sigma w)_i $ is the $i$-th component of the vector $ \\Sigma w $.\n",
    "\n",
    "**Gradient Descent Update Rule**\n",
    "\n",
    "The gradient descent update rule for $ w_i $ is:\n",
    "\n",
    "$$\n",
    "w_i^{\\text{new}} = w_i^{\\text{old}} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "where $ \\alpha $ is the learning rate.\n",
    "\n",
    "Substitute the gradient:\n",
    "\n",
    "$$\n",
    "w_i^{\\text{new}} = w_i^{\\text{old}} - \\alpha \\left( 2 (\\Sigma w)_i - \\nu (\\ln w_i + 1) \\right)\n",
    "$$\n",
    "\n",
    "**Apply the Constraint**\n",
    "\n",
    "To enforce the constraint $ \\sum_{i=1}^{L} w_i = 1 $, use a projection step:\n",
    "\n",
    "After each update, normalize the weights to ensure they sum to 1:\n",
    "\n",
    "$$\n",
    "w_i^{\\text{new}} = \\frac{w_i^{\\text{new}}}{\\sum_{j=1}^{L} w_j^{\\text{new}}}\n",
    "$$\n",
    "\n",
    "**Iterate**\n",
    "\n",
    "Repeat the update and normalization steps until convergence:\n",
    "\n",
    "- **Compute the Gradient**: Calculate the gradient for each weight.\n",
    "- **Update Weights**: Apply the gradient descent update rule.\n",
    "- **Normalize Weights**: Ensure weights sum to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Gini-index orness and Diagonal $ \\Sigma $-norm\n",
    "\n",
    "### Problem Restatement\n",
    "\n",
    "You want to minimize:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) = w^T \\Sigma w - \\nu \\left(\\sum_{i=1}^{L} w_i - \\sum_{i=1}^{L} w_i^2\\right)\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{L} w_i = 1\n",
    "$$\n",
    "\n",
    "where $ \\Sigma $ is a diagonal matrix with diagonal elements $ \\sigma_{ii}^2 $.\n",
    "\n",
    "### Lagrangian Function\n",
    "\n",
    "Define the Lagrangian function including the constraint:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w, \\lambda) = w^T \\Sigma w - \\nu \\left(\\sum_{i=1}^{L} w_i - \\sum_{i=1}^{L} w_i^2\\right) + \\lambda \\left(\\sum_{i=1}^{L} w_i - 1\\right)\n",
    "$$\n",
    "\n",
    "### Gradient Calculation\n",
    "\n",
    "**Compute the Gradient**\n",
    "\n",
    "The diagonal covariance matrix $ \\Sigma $ means that $ \\Sigma $ is a diagonal matrix with elements $ \\sigma_{ii}^2 $. The matrix-vector multiplication $ w^T \\Sigma w $ simplifies to:\n",
    "\n",
    "$$\n",
    "w^T \\Sigma w = \\sum_{i=1}^{L} \\sigma_{ii}^2 w_i^2\n",
    "$$\n",
    "\n",
    "The Lagrangian function simplifies to:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w, \\lambda) = \\sum_{i=1}^{L} \\sigma_{ii}^2 w_i^2 - \\nu \\left(\\sum_{i=1}^{L} w_i - \\sum_{i=1}^{L} w_i^2\\right) + \\lambda \\left(\\sum_{i=1}^{L} w_i - 1\\right)\n",
    "$$\n",
    "\n",
    "Take the partial derivative of $ \\mathcal{L} $ with respect to $ w_i $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_i} = 2 \\sigma_{ii}^2 w_i - \\nu (1 - 2 w_i) + \\lambda\n",
    "$$\n",
    "\n",
    "Set the gradient to zero:\n",
    "\n",
    "$$\n",
    "2 \\sigma_{ii}^2 w_i - \\nu + 2 \\nu w_i + \\lambda = 0\n",
    "$$\n",
    "\n",
    "Rearrange to solve for $ w_i $:\n",
    "\n",
    "$$\n",
    "2 \\sigma_{ii}^2 w_i + 2 \\nu w_i = \\nu - \\lambda\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_i = \\frac{\\nu - \\lambda}{2 (\\sigma_{ii}^2 + \\nu)}\n",
    "$$\n",
    "\n",
    "**Apply the Constraint**\n",
    "\n",
    "To ensure the constraint $ \\sum_{i=1}^{L} w_i = 1 $ is satisfied:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{L} w_i = \\sum_{i=1}^{L} \\frac{\\nu - \\lambda}{2 (\\sigma_{ii}^2 + \\nu)} = 1\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "\\frac{L (\\nu - \\lambda)}{2 \\sum_{i=1}^{L} (\\sigma_{ii}^2 + \\nu)} = 1\n",
    "$$\n",
    "\n",
    "Solve for $ \\lambda $:\n",
    "\n",
    "$$\n",
    "L (\\nu - \\lambda) = 2 \\sum_{i=1}^{L} (\\sigma_{ii}^2 + \\nu)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nu - \\lambda = \\frac{2 \\sum_{i=1}^{L} (\\sigma_{ii}^2 + \\nu)}{L}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda = \\nu - \\frac{2 \\sum_{i=1}^{L} (\\sigma_{ii}^2 + \\nu)}{L}\n",
    "$$\n",
    "\n",
    "### Substitute $ \\lambda $ Back\n",
    "\n",
    "Substitute $ \\lambda $ back into the expression for $ w_i $:\n",
    "\n",
    "$$\n",
    "w_i = \\frac{\\nu - \\left[\\nu - \\frac{2 \\sum_{i=1}^{L} (\\sigma_{ii}^2 + \\nu)}{L}\\right]}{2 (\\sigma_{ii}^2 + \\nu)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_i = \\frac{\\frac{2 \\sum_{i=1}^{L} (\\sigma_{ii}^2 + \\nu)}{L}}{2 (\\sigma_{ii}^2 + \\nu)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_i = \\frac{\\sum_{i=1}^{L} (\\sigma_{ii}^2 + \\nu)}{L (\\sigma_{ii}^2 + \\nu)}\n",
    "$$\n",
    "\n",
    "### Final Solution\n",
    "\n",
    "Thus, the optimal weight for each $ w_i $ is:\n",
    "\n",
    "$$\n",
    "w_i = \\frac{\\sum_{i=1}^{L} (\\sigma_{ii}^2 + \\nu)}{L (\\sigma_{ii}^2 + \\nu)}\n",
    "$$\n",
    "\n",
    "Again weight of $i^\\text{th}$-classifier proportional of normalized inverse of variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Templates\n",
    "\n",
    "### Decision Template $ DT_j $\n",
    "\n",
    "![DT_j](../EnsembleLearning/EnsembleLearningImage/DT_Set_J.png)\n",
    "\n",
    "1. **Definition**:\n",
    "   - **Decision Profile $ DP(x_i) $**: For each sample $ x_i $ belonging to class $ \\omega_j $, $ DP(x_i) $ is an $ L \\times C $ matrix where $ L $ is the number of classifiers, and $ C $ is the number of classes. Each entry $ DP_{i,c}(x_i) $ in this matrix represents the decision score or probability assigned by the $ i $-th classifier to the $ c $-th class for the sample $ x_i $.\n",
    "\n",
    "   - **Decision Template $ DT_j $**: The decision template $ DT_j $ is a consolidated $ L \\times C $ matrix that summarizes the decision profiles of all samples in class $ \\omega_j $.\n",
    "\n",
    "2. **Aggregation**:\n",
    "   - **Set of Decision Profiles**: For class $ \\omega_j $, you have $ k_j $ samples. Each sample $ x_i $ in $ \\omega_j $ provides a decision profile $ DP(x_i) $, which is an $ L \\times C $ matrix. Therefore, $ DT_j $ involves aggregating these $ k_j $ matrices.\n",
    "\n",
    "   - **Mean Operator**: To compute $ DT_j $, apply the mean operator to aggregate the decision profiles across all samples in class $ \\omega_j $. Specifically, if $ DP(x_i) $ is the decision profile matrix for sample $ x_i $, the decision template $ DT_j $ is computed as:\n",
    "  \n",
    "$$\n",
    "  DT_j = \\frac{1}{k_j} \\sum_{i=1}^{k_j} DP(x_i)\n",
    "$$\n",
    "\n",
    "where $ k_j $ is the number of samples in class $ \\omega_j $.\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "Assume you have 3 classifiers and 4 classes. For class $ \\omega_j $, consider the following decision profiles for 3 samples:\n",
    "\n",
    "- **Decision Profile for Sample 1**:\n",
    "  $$\n",
    "  DP(x_1) =\n",
    "  \\begin{bmatrix}\n",
    "  0.2 & 0.5 & 0.2 & 0.1 \\\\\n",
    "  0.3 & 0.4 & 0.1 & 0.2 \\\\\n",
    "  0.1 & 0.3 & 0.4 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Decision Profile for Sample 2**:\n",
    "  $$\n",
    "  DP(x_2) =\n",
    "  \\begin{bmatrix}\n",
    "  0.3 & 0.4 & 0.1 & 0.2 \\\\\n",
    "  0.2 & 0.5 & 0.2 & 0.1 \\\\\n",
    "  0.2 & 0.3 & 0.4 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Decision Profile for Sample 3**:\n",
    "  $$\n",
    "  DP(x_3) =\n",
    "  \\begin{bmatrix}\n",
    "  0.1 & 0.3 & 0.4 & 0.2 \\\\\n",
    "  0.4 & 0.3 & 0.2 & 0.1 \\\\\n",
    "  0.3 & 0.2 & 0.4 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "**Compute Decision Template $ DT_j $**:\n",
    "$$\n",
    "DT_j = \\frac{1}{3} \\left( DP(x_1) + DP(x_2) + DP(x_3) \\right)\n",
    "$$\n",
    "$$\n",
    "DT_j = \\frac{1}{3}\n",
    "\\begin{bmatrix}\n",
    "0.2+0.3+0.1 & 0.5+0.4+0.3 & 0.2+0.1+0.4 & 0.1+0.2+0.2 \\\\\n",
    "0.3+0.2+0.4 & 0.4+0.5+0.3 & 0.1+0.2+0.4 & 0.2+0.1+0.1 \\\\\n",
    "0.1+0.2+0.3 & 0.3+0.3+0.2 & 0.4+0.4+0.4 & 0.2+0.1+0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "DT_j = \\frac{1}{3}\n",
    "\\begin{bmatrix}\n",
    "0.6 & 1.2 & 0.7 & 0.5 \\\\\n",
    "0.9 & 1.2 & 0.7 & 0.4 \\\\\n",
    "0.6 & 0.8 & 1.2 & 0.4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "DT_j =\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.4 & 0.233 & 0.167 \\\\\n",
    "0.3 & 0.4 & 0.233 & 0.133 \\\\\n",
    "0.2 & 0.267 & 0.4 & 0.133\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Template as a Normal Distribution\n",
    "\n",
    "**Decision Profiles Set**:\n",
    "   Let $ S_{DP_j} $ be the set of decision profiles for all samples in class $ \\omega_j $:\n",
    "   $$\n",
    "   S_{DP_j} = \\{ DP(x_k) \\mid x_k \\in \\omega_j \\}\n",
    "   $$\n",
    "   where each $ DP(x_k) $ is an $ L \\times C $ matrix.\n",
    "\n",
    "**Mean Decision Profile**:\n",
    "   Compute the mean decision profile matrix $ \\mu_j $, which is the average of all decision profiles in $ S_{DP_j} $:\n",
    "   $$\n",
    "   \\mu_j = \\frac{1}{k_j} \\sum_{k=1}^{k_j} DP(x_k)\n",
    "   $$\n",
    "   Here, $ \\mu_j $ is an $ L \\times C $ matrix representing the mean decision profile for class $ \\omega_j $.\n",
    "\n",
    "**Covariance Matrix**:\n",
    "   Compute the covariance matrix $ \\Sigma_j $ for the decision profiles in $ S_{DP_j} $. \n",
    "\n",
    "   - **Mean matrix**:\n",
    "$$\n",
    "\\bar{\\textbf{DP}}_j = \\frac{1}{k_j} \\sum_{k=1}^{k_j} \\textbf{DP}_k\n",
    "$$\n",
    "\n",
    "\n",
    "- **Covariance Matrix**:\n",
    "\n",
    "$$\n",
    "\\Sigma_j = \\frac{1}{k_j - 1} \\sum_{k=1}^{k_j} (\\textbf{DP}_k - \\bar{\\textbf{DP}}_j) (\\textbf{DP}_k - \\bar{\\textbf{DP}}_j)^T\n",
    "$$\n",
    "\n",
    "### Normal Distribution Representation\n",
    "\n",
    "Given the mean matrix $ \\mu_j $ and covariance matrix $ \\Sigma_j $, we can represent the decision template $ DT_j $ as a multivariate normal distribution:\n",
    "$$\n",
    "DT_j \\sim \\mathcal{N}(\\mu_j, \\Sigma_j)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mu_j $ is the $ L \\times C $ mean decision profile matrix.\n",
    "- $ \\Sigma_j $ is the $ L \\times L $ covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainin and operation using Decision Templates\n",
    "\n",
    "#### Training\n",
    "\n",
    "**Constructing Decision Templates**:\n",
    "\n",
    "   For each class $ \\omega_j $ (where $ j = 1, \\ldots, C $), we need to compute the decision template $ DT_j $. This involves the following steps:\n",
    "\n",
    "   - **Collect Decision Profiles**:\n",
    "     For each sample $ z_k $ in class $ \\omega_j $, obtain its decision profile matrix $ DP(z_k) $. The decision profile $ DP(z_k) $ is an $ L \\times C $ matrix, where $ L $ is the number of classifiers and $ C $ is the number of classes.\n",
    "\n",
    "   - **Calculate the Mean Decision Profile**:\n",
    "     Compute the mean decision profile $ DT_j $ for class $ \\omega_j $ as follows:\n",
    "     $$\n",
    "     DT_j = \\frac{1}{N_j} \\sum_{z_k \\in \\omega_j} DP(z_k)\n",
    "     $$\n",
    "     Here:\n",
    "     - $ N_j $ is the number of samples in class $ \\omega_j $.\n",
    "     - $ DP(z_k) $ is the decision profile matrix of sample $ z_k $.\n",
    "     - $ DT_j $ is the $ L \\times C $ mean matrix representing the average decision profile for class $ \\omega_j $.\n",
    "\n",
    "#### Operation\n",
    "\n",
    "**Classifying New Samples**:\n",
    "\n",
    "   Given a new input sample $ x \\in \\mathbb{R}^n $, we want to classify it using the decision templates.\n",
    "\n",
    "   - **Construct Decision Profile**:\n",
    "     Compute the decision profile $ DP(x) $ for the input sample $ x $. This is an $ L \\times C $ matrix where each entry represents the classifier's decision for each class.\n",
    "\n",
    "   - **Calculate Similarity**:\n",
    "     Measure the similarity between the decision profile $ DP(x) $ and each decision template $ DT_j $. Various similarity measures can be used, such as Euclidean distance, cosine similarity, etc. Let $ S(DP(x), DT_j) $ denote the similarity between $ DP(x) $ and $ DT_j $.\n",
    "\n",
    "   - **Compute Membership Scores**:\n",
    "     For each class $ j $, compute the membership score $ m_j(x) $ as:\n",
    "\n",
    "$$\n",
    "m_j(x) = S(DP(x), DT_j) \\quad \\text{for } j = 1, \\ldots, C\n",
    "$$\n",
    "Here, $ m_j(x) $ (support of class $\\omega_j$ ) represents how similar the decision profile of $ x $ is to the decision template of class $ \\omega_j $.\n",
    "\n",
    "   - **Class Decision**:\n",
    "     Assign the class label $ \\omega_j $ to the input sample $ x $ based on the highest membership score:\n",
    "     \n",
    "$$\n",
    "\\hat{y} = \\arg \\max_{j} m_j(x)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miniproject: Ensemble Diversity\n",
    "Ensemble Diversity refers to the differences among the individual models or learners in an ensemble. The idea is that for an ensemble method to improve performance over any single learner, the individual models in the ensemble must exhibit diversity. If all the models in the ensemble make the same predictions, combining them will not yield any performance improvement over using a single model.\n",
    "\n",
    "- Complete this chapter with code and analytical notes\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
